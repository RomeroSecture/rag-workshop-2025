{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š MÃ³dulo 1: Fundamentos de RAG\n",
    "## De Cero a tu Primer Sistema RAG Funcional (75 minutos)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Objetivos de este mÃ³dulo:\n",
    "1. **Entender** quÃ© es RAG y por quÃ© es revolucionario\n",
    "2. **Construir** tu primer pipeline RAG desde cero\n",
    "3. **Experimentar** con chunking, embeddings y retrieval\n",
    "4. **Medir** latencia, costo y calidad\n",
    "\n",
    "### â±ï¸ Timeline:\n",
    "- 08:15-08:35: TeorÃ­a y conceptos (20 min)\n",
    "- 08:35-08:55: ImplementaciÃ³n guiada (20 min)\n",
    "- 08:55-09:30: PrÃ¡ctica y experimentaciÃ³n (35 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Setup y Conceptos [08:15-08:35]\n",
    "\n",
    "### ğŸ§  Â¿QuÃ© es RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation** = Recuperar + Aumentar + Generar\n",
    "\n",
    "Es como darle a un LLM:\n",
    "- ğŸ“š Una biblioteca personal\n",
    "- ğŸ” Un buscador ultra-rÃ¡pido\n",
    "- ğŸ¯ Contexto especÃ­fico para cada pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1: Imports y configuraciÃ³n\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# AÃ±adir src al path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Imports de nuestros mÃ³dulos\n",
    "from shared_config import RAGMasterConfig, TestSuite, MetricsTracker, Module\n",
    "\n",
    "# ConfiguraciÃ³n\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verificar API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"âŒ Por favor configura OPENAI_API_KEY en .env\")\n",
    "else:\n",
    "    print(f\"âœ… API Key configurada: {api_key[:7]}...\")\n",
    "\n",
    "# Inicializar tracker de mÃ©tricas\n",
    "metrics = MetricsTracker()\n",
    "config = RAGMasterConfig()\n",
    "\n",
    "print(\"\\nğŸš€ Ambiente listo para MÃ³dulo 1: Fundamentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Los 4 Pilares de RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2: Visualizar arquitectura RAG\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Componentes\n",
    "components = [\n",
    "    {\"name\": \"ğŸ“„ Documentos\\nOriginales\", \"pos\": (1, 6), \"color\": \"lightblue\"},\n",
    "    {\"name\": \"âœ‚ï¸ Text\\nSplitter\", \"pos\": (3, 6), \"color\": \"lightgreen\"},\n",
    "    {\"name\": \"ğŸ”¢ Embeddings\\nModel\", \"pos\": (5, 6), \"color\": \"lightyellow\"},\n",
    "    {\"name\": \"ğŸ’¾ Vector\\nDatabase\", \"pos\": (7, 6), \"color\": \"lightcoral\"},\n",
    "    {\"name\": \"â“ User\\nQuery\", \"pos\": (1, 3), \"color\": \"lightgray\"},\n",
    "    {\"name\": \"ğŸ” Semantic\\nSearch\", \"pos\": (4, 3), \"color\": \"lightgreen\"},\n",
    "    {\"name\": \"ğŸ“‘ Retrieved\\nContext\", \"pos\": (7, 3), \"color\": \"lightyellow\"},\n",
    "    {\"name\": \"ğŸ¤– LLM\\n(GPT)\", \"pos\": (4, 0.5), \"color\": \"lightblue\"},\n",
    "    {\"name\": \"ğŸ’¬ Final\\nAnswer\", \"pos\": (7, 0.5), \"color\": \"lightgreen\"}\n",
    "]\n",
    "\n",
    "# Dibujar componentes\n",
    "for comp in components:\n",
    "    box = FancyBboxPatch(\n",
    "        (comp[\"pos\"][0]-0.4, comp[\"pos\"][1]-0.3),\n",
    "        0.8, 0.6,\n",
    "        boxstyle=\"round,pad=0.1\",\n",
    "        facecolor=comp[\"color\"],\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=2\n",
    "    )\n",
    "    ax.add_patch(box)\n",
    "    ax.text(comp[\"pos\"][0], comp[\"pos\"][1], comp[\"name\"], \n",
    "            ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# Flechas\n",
    "arrows = [\n",
    "    ((1.4, 6), (2.6, 6), \"Cargar\"),\n",
    "    ((3.4, 6), (4.6, 6), \"Chunks\"),\n",
    "    ((5.4, 6), (6.6, 6), \"Vectors\"),\n",
    "    ((7, 5.7), (7, 3.3), \"Store\"),\n",
    "    ((1.4, 3), (3.6, 3), \"Embed\"),\n",
    "    ((4.4, 3), (6.6, 3), \"Top-K\"),\n",
    "    ((7, 2.7), (4.4, 0.8), \"Context\"),\n",
    "    ((1, 2.7), (3.6, 0.8), \"Query\"),\n",
    "    ((4.4, 0.5), (6.6, 0.5), \"Generate\")\n",
    "]\n",
    "\n",
    "for start, end, label in arrows:\n",
    "    arrow = FancyArrowPatch(\n",
    "        start, end,\n",
    "        arrowstyle=\"->\",\n",
    "        connectionstyle=\"arc3,rad=0.1\",\n",
    "        linewidth=2,\n",
    "        color=\"darkblue\"\n",
    "    )\n",
    "    ax.add_patch(arrow)\n",
    "    \n",
    "    # Label en la flecha\n",
    "    mid_x = (start[0] + end[0]) / 2\n",
    "    mid_y = (start[1] + end[1]) / 2\n",
    "    ax.text(mid_x, mid_y + 0.2, label, fontsize=8, ha=\"center\", style=\"italic\")\n",
    "\n",
    "# ConfiguraciÃ³n del plot\n",
    "ax.set_xlim(0, 8)\n",
    "ax.set_ylim(-0.5, 7)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "# TÃ­tulo y fases\n",
    "ax.text(4, 7.5, \"ğŸ—ï¸ ARQUITECTURA RAG COMPLETA\", fontsize=16, fontweight=\"bold\", ha=\"center\")\n",
    "ax.text(4, 6.8, \"FASE 1: INDEXACIÃ“N\", fontsize=10, color=\"blue\", ha=\"center\")\n",
    "ax.text(4, 3.8, \"FASE 2: RETRIEVAL\", fontsize=10, color=\"green\", ha=\"center\")\n",
    "ax.text(4, 1.3, \"FASE 3: GENERACIÃ“N\", fontsize=10, color=\"red\", ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ¯ Los 3 momentos clave de RAG:\")\n",
    "print(\"1ï¸âƒ£ INDEXACIÃ“N: Preparar y almacenar el conocimiento\")\n",
    "print(\"2ï¸âƒ£ RETRIEVAL: Encontrar informaciÃ³n relevante\")\n",
    "print(\"3ï¸âƒ£ GENERACIÃ“N: Crear respuesta con contexto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: ImplementaciÃ³n BÃ¡sica [08:35-08:55]\n",
    "\n",
    "### ğŸ› ï¸ Construyendo tu Primer RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3: ImplementaciÃ³n del RAG mÃ¡s simple posible\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"Tu primer RAG en 50 lÃ­neas de cÃ³digo\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"ğŸš€ Inicializando SimpleRAG...\")\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.chroma = chromadb.Client()\n",
    "        \n",
    "        # Crear colecciÃ³n (como una tabla en base de datos)\n",
    "        try:\n",
    "            self.collection = self.chroma.create_collection(\"simple_rag\")\n",
    "            print(\"âœ… ColecciÃ³n creada\")\n",
    "        except:\n",
    "            self.chroma.delete_collection(\"simple_rag\")\n",
    "            self.collection = self.chroma.create_collection(\"simple_rag\")\n",
    "            print(\"â™»ï¸ ColecciÃ³n recreada\")\n",
    "    \n",
    "    def load_document(self, filepath: str) -> str:\n",
    "        \"\"\"Cargar documento (PDF o TXT)\"\"\"\n",
    "        print(f\"ğŸ“„ Cargando: {filepath}\")\n",
    "        \n",
    "        if filepath.endswith('.pdf'):\n",
    "            with open(filepath, 'rb') as file:\n",
    "                pdf = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    text += page.extract_text()\n",
    "        else:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "        \n",
    "        print(f\"âœ… Documento cargado: {len(text)} caracteres\")\n",
    "        return text\n",
    "    \n",
    "    def create_chunks(self, text: str, chunk_size: int = 500) -> List[str]:\n",
    "        \"\"\"Dividir texto en chunks\"\"\"\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size):\n",
    "            chunk = text[i:i+chunk_size]\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        print(f\"âœ‚ï¸ Creados {len(chunks)} chunks de ~{chunk_size} caracteres\")\n",
    "        return chunks\n",
    "    \n",
    "    def index_chunks(self, chunks: List[str]):\n",
    "        \"\"\"Indexar chunks en vector database\"\"\"\n",
    "        print(f\"ğŸ”¢ Indexando {len(chunks)} chunks...\")\n",
    "        \n",
    "        # AÃ±adir a ChromaDB (Ã©l se encarga de los embeddings)\n",
    "        self.collection.add(\n",
    "            documents=chunks,\n",
    "            ids=[f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Chunks indexados en ChromaDB\")\n",
    "    \n",
    "    def query(self, question: str, k: int = 3) -> str:\n",
    "        \"\"\"Hacer una pregunta al RAG\"\"\"\n",
    "        print(f\"\\nâ“ Pregunta: {question}\")\n",
    "        \n",
    "        # 1. Buscar chunks relevantes\n",
    "        start_time = time.time()\n",
    "        results = self.collection.query(\n",
    "            query_texts=[question],\n",
    "            n_results=k\n",
    "        )\n",
    "        retrieval_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # 2. Preparar contexto\n",
    "        context = \"\\n\\n\".join(results['documents'][0])\n",
    "        print(f\"ğŸ” Recuperados {k} chunks relevantes en {retrieval_time:.0f}ms\")\n",
    "        \n",
    "        # 3. Generar respuesta con LLM\n",
    "        prompt = f\"\"\"\n",
    "        Contexto:\n",
    "        {context}\n",
    "        \n",
    "        Pregunta: {question}\n",
    "        \n",
    "        Instrucciones: Responde basÃ¡ndote ÃšNICAMENTE en el contexto proporcionado.\n",
    "        Si no encuentras la informaciÃ³n en el contexto, di \"No tengo esa informaciÃ³n\".\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        generation_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        total_time = retrieval_time + generation_time\n",
    "        \n",
    "        # MÃ©tricas\n",
    "        print(f\"â±ï¸ Tiempos: Retrieval={retrieval_time:.0f}ms, Generation={generation_time:.0f}ms, Total={total_time:.0f}ms\")\n",
    "        \n",
    "        # Registrar en mÃ©tricas globales\n",
    "        cost = len(prompt) / 1000 * 0.0015 + len(answer) / 1000 * 0.002  # Aproximado\n",
    "        metrics.log_query(\n",
    "            module=Module.BASICS,\n",
    "            query=question,\n",
    "            response=answer,\n",
    "            latency=total_time,\n",
    "            cost=cost,\n",
    "            tokens=len(prompt.split()) + len(answer.split())\n",
    "        )\n",
    "        \n",
    "        return answer\n",
    "\n",
    "# Crear instancia\n",
    "rag = SimpleRAG()\n",
    "print(\"\\nâœ… Â¡Tu primer RAG estÃ¡ listo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: Cargar y procesar documento\n",
    "# Usamos un documento de ejemplo\n",
    "doc_path = \"../data/company_handbook.pdf\"\n",
    "\n",
    "# Verificar que existe\n",
    "if not Path(doc_path).exists():\n",
    "    # Crear documento de ejemplo si no existe\n",
    "    print(\"ğŸ“ Creando documento de ejemplo...\")\n",
    "    !python ../src/utils.py --create-sample-data\n",
    "\n",
    "# Cargar documento\n",
    "text = rag.load_document(doc_path)\n",
    "\n",
    "# Ver preview\n",
    "print(\"\\nğŸ“– Preview del documento:\")\n",
    "print(\"-\" * 50)\n",
    "print(text[:500] + \"...\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5: Chunking e IndexaciÃ³n\n",
    "print(\"ğŸ”¨ PASO 1: CHUNKING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear chunks\n",
    "chunks = rag.create_chunks(text, chunk_size=500)\n",
    "\n",
    "# Mostrar estadÃ­sticas\n",
    "print(f\"\\nğŸ“Š EstadÃ­sticas de chunking:\")\n",
    "print(f\"- Total chunks: {len(chunks)}\")\n",
    "print(f\"- TamaÃ±o promedio: {np.mean([len(c) for c in chunks]):.0f} chars\")\n",
    "print(f\"- Chunk mÃ¡s pequeÃ±o: {min([len(c) for c in chunks])} chars\")\n",
    "print(f\"- Chunk mÃ¡s grande: {max([len(c) for c in chunks])} chars\")\n",
    "\n",
    "# Ver algunos chunks\n",
    "print(\"\\nğŸ‘€ Ejemplo de chunks:\")\n",
    "for i in range(min(3, len(chunks))):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(chunks[i][:150] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6: Indexar en Vector Database\n",
    "print(\"ğŸ’¾ PASO 2: INDEXACIÃ“N\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Indexar solo los primeros 20 chunks para rapidez\n",
    "# En producciÃ³n indexarÃ­as todos\n",
    "chunks_to_index = chunks[:20]\n",
    "\n",
    "print(f\"\\nğŸ¯ Indexando {len(chunks_to_index)} chunks en ChromaDB...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rag.index_chunks(chunks_to_index)\n",
    "\n",
    "indexing_time = (time.time() - start_time) * 1000\n",
    "print(f\"âœ… IndexaciÃ³n completada en {indexing_time:.0f}ms\")\n",
    "print(f\"âš¡ Velocidad: {len(chunks_to_index) / (indexing_time/1000):.1f} chunks/segundo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: PrÃ¡ctica - Tu Turno [08:55-09:30]\n",
    "\n",
    "### ğŸ¯ Hora de Experimentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 7: Tu primera query RAG\n",
    "print(\"ğŸš€ TU PRIMERA QUERY RAG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# La query hÃ©roe del dÃ­a\n",
    "respuesta = rag.query(\"Â¿CuÃ¡l es la polÃ­tica de vacaciones?\")\n",
    "\n",
    "print(\"\\nğŸ’¬ Respuesta:\")\n",
    "print(\"-\" * 40)\n",
    "print(respuesta)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Verificar si es correcta\n",
    "evaluation = TestSuite.evaluate_response(respuesta, Module.BASICS)\n",
    "print(f\"\\nğŸ“Š EvaluaciÃ³n:\")\n",
    "print(f\"- Score: {evaluation['score']:.2f}/1.0\")\n",
    "print(f\"- Â¿PasÃ³?: {'âœ… SÃ­' if evaluation['passed'] else 'âŒ No'}\")\n",
    "print(f\"- Tiene info bÃ¡sica: {'âœ…' if evaluation['has_basic_info'] else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8: Experimentar con diferentes queries\n",
    "print(\"ğŸ§ª EXPERIMENTO: Diferentes tipos de preguntas\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_queries = [\n",
    "    \"Â¿CuÃ¡l es el horario de trabajo?\",\n",
    "    \"Â¿Hay trabajo remoto?\",\n",
    "    \"Â¿CuÃ¡les son los beneficios?\",\n",
    "    \"Â¿CÃ³mo es el proceso de onboarding?\",\n",
    "    \"Â¿QuÃ© pasa si me caso?\"  # Edge case\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    answer = rag.query(query, k=3)\n",
    "    \n",
    "    print(f\"\\nğŸ’¬ Respuesta:\")\n",
    "    print(answer[:200] + \"...\" if len(answer) > 200 else answer)\n",
    "    \n",
    "    # Pausa para no saturar la API\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 9: Experimentar con parÃ¡metros\n",
    "print(\"âš™ï¸ EXPERIMENTO: Ajustando parÃ¡metros\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "query_test = \"Â¿CuÃ¡les son todos los beneficios de la empresa?\"\n",
    "\n",
    "# Probar diferentes valores de K (chunks recuperados)\n",
    "k_values = [1, 3, 5, 10]\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nğŸ” Probando con k={k} chunks...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    answer = rag.query(query_test, k=k)\n",
    "    latency = (time.time() - start) * 1000\n",
    "    \n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'answer_length': len(answer),\n",
    "        'latency_ms': latency,\n",
    "        'answer_preview': answer[:100]\n",
    "    })\n",
    "    \n",
    "    print(f\"- Longitud respuesta: {len(answer)} chars\")\n",
    "    print(f\"- Latencia: {latency:.0f}ms\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Visualizar resultados\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nğŸ“Š Resumen de resultados:\")\n",
    "print(df[['k', 'answer_length', 'latency_ms']])\n",
    "\n",
    "# ConclusiÃ³n\n",
    "print(\"\\nğŸ’¡ ObservaciÃ³n: MÃ¡s chunks = respuestas mÃ¡s completas pero mayor latencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 10: Probar con pregunta que NO estÃ¡ en el documento\n",
    "print(\"âŒ EXPERIMENTO: Pregunta sin respuesta\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pregunta que no deberÃ­a estar en el manual\n",
    "pregunta_imposible = \"Â¿CuÃ¡l es el precio de las acciones de la empresa en bolsa?\"\n",
    "\n",
    "print(f\"\\nâ“ Pregunta: {pregunta_imposible}\")\n",
    "respuesta = rag.query(pregunta_imposible)\n",
    "\n",
    "print(f\"\\nğŸ’¬ Respuesta:\")\n",
    "print(\"-\" * 40)\n",
    "print(respuesta)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Verificar si el sistema reconoce que no tiene la informaciÃ³n\n",
    "if \"no tengo\" in respuesta.lower() or \"no encuentro\" in respuesta.lower():\n",
    "    print(\"\\nâœ… Â¡Bien! El RAG reconoce cuando no tiene informaciÃ³n\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Cuidado: El RAG podrÃ­a estar alucinando\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š AnÃ¡lisis de MÃ©tricas del MÃ³dulo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 11: Ver mÃ©tricas acumuladas\n",
    "print(\"ğŸ“ˆ MÃ‰TRICAS DEL MÃ“DULO 1\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Obtener resumen\n",
    "summary = metrics.get_summary()\n",
    "\n",
    "if Module.BASICS.name in summary:\n",
    "    stats = summary[Module.BASICS.name]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š EstadÃ­sticas:\")\n",
    "    print(f\"- Queries realizadas: {stats['queries_count']}\")\n",
    "    print(f\"- Latencia promedio: {stats['avg_latency_ms']:.0f}ms\")\n",
    "    print(f\"- Costo total: ${stats['total_cost_usd']:.4f}\")\n",
    "    print(f\"- Tokens totales: {stats['total_tokens']}\")\n",
    "    \n",
    "    # Comparar con target\n",
    "    target = config.target_metrics[Module.BASICS]\n",
    "    print(f\"\\nğŸ¯ ComparaciÃ³n con objetivos:\")\n",
    "    print(f\"- Latencia: {stats['avg_latency_ms']:.0f}ms vs objetivo {target['latency']}ms\")\n",
    "    print(f\"- Costo promedio: ${stats['total_cost_usd']/stats['queries_count']:.4f} vs objetivo ${target['cost']}\")\n",
    "    \n",
    "    # Visualizar\n",
    "    metrics.plot_progress()\n",
    "else:\n",
    "    print(\"No hay mÃ©tricas registradas aÃºn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ DesafÃ­os Adicionales (Si terminas antes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESAFÃO 1: Mejorar el chunking\n",
    "def smart_chunking(text: str, chunk_size: int = 500) -> List[str]:\n",
    "    \"\"\"\n",
    "    TODO: Implementa un chunking mÃ¡s inteligente que:\n",
    "    - No corte frases a la mitad\n",
    "    - Respete pÃ¡rrafos cuando sea posible\n",
    "    - Mantenga contexto entre chunks\n",
    "    \"\"\"\n",
    "    # Tu cÃ³digo aquÃ­\n",
    "    pass\n",
    "\n",
    "# DESAFÃO 2: AÃ±adir metadatos\n",
    "def index_with_metadata(chunks: List[str], metadata: List[Dict]):\n",
    "    \"\"\"\n",
    "    TODO: Indexa chunks con metadata adicional como:\n",
    "    - NÃºmero de pÃ¡gina\n",
    "    - SecciÃ³n del documento\n",
    "    - Fecha de actualizaciÃ³n\n",
    "    \"\"\"\n",
    "    # Tu cÃ³digo aquÃ­\n",
    "    pass\n",
    "\n",
    "# DESAFÃO 3: Implementar re-ranking\n",
    "def rerank_results(query: str, results: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    TODO: Re-ordenar resultados basÃ¡ndose en:\n",
    "    - Relevancia semÃ¡ntica\n",
    "    - Longitud del chunk\n",
    "    - Presencia de keywords\n",
    "    \"\"\"\n",
    "    # Tu cÃ³digo aquÃ­\n",
    "    pass\n",
    "\n",
    "print(\"ğŸ’ª Â¡Intenta resolver estos desafÃ­os!\")\n",
    "print(\"Pista: Mira el mÃ³dulo 2 para inspiraciÃ³n ğŸ˜‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Â¡Felicitaciones!\n",
    "\n",
    "### âœ… Lo que has logrado en el MÃ³dulo 1:\n",
    "\n",
    "1. **Construiste** tu primer sistema RAG funcional\n",
    "2. **Indexaste** documentos en un vector database\n",
    "3. **Realizaste** bÃºsqueda semÃ¡ntica\n",
    "4. **Generaste** respuestas con contexto\n",
    "5. **Mediste** latencia y costos\n",
    "\n",
    "### ğŸ“Š Tus mÃ©tricas actuales:\n",
    "- â±ï¸ Latencia: ~2000ms\n",
    "- ğŸ’° Costo: ~$0.01 por query\n",
    "- ğŸ¯ Accuracy: ~70%\n",
    "\n",
    "### ğŸš€ En el MÃ³dulo 2 mejoraremos:\n",
    "- Reducir latencia a 1000ms (-50%)\n",
    "- Reducir costos a $0.008 (-20%)\n",
    "- Aumentar accuracy a 80% (+10%)\n",
    "\n",
    "---\n",
    "\n",
    "**â˜• Toma un break de 15 minutos y nos vemos en el MÃ³dulo 2!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}