{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö M√≥dulo 1: Fundamentos de RAG\n",
    "## De Cero a tu Primer Sistema RAG Funcional (75 minutos)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objetivos de este m√≥dulo:\n",
    "1. **Entender** qu√© es RAG y por qu√© es revolucionario\n",
    "2. **Construir** tu primer pipeline RAG desde cero\n",
    "3. **Experimentar** con chunking, embeddings y retrieval\n",
    "4. **Medir** latencia, costo y calidad\n",
    "\n",
    "### ‚è±Ô∏è Timeline:\n",
    "- 08:15-08:35: Teor√≠a y conceptos (20 min)\n",
    "- 08:35-08:55: Implementaci√≥n guiada (20 min)\n",
    "- 08:55-09:30: Pr√°ctica y experimentaci√≥n (35 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Setup y Conceptos [08:15-08:35]\n",
    "\n",
    "### üß† ¬øQu√© es RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation** = Recuperar + Aumentar + Generar\n",
    "\n",
    "Es como darle a un LLM:\n",
    "- üìö Una biblioteca personal\n",
    "- üîç Un buscador ultra-r√°pido\n",
    "- üéØ Contexto espec√≠fico para cada pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1: Imports y configuraci√≥n\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# A√±adir src al path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Imports de nuestros m√≥dulos\n",
    "from shared_config import RAGMasterConfig, TestSuite, MetricsTracker, Module\n",
    "\n",
    "# Configuraci√≥n\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verificar API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"‚ùå Por favor configura OPENAI_API_KEY en .env\")\n",
    "else:\n",
    "    print(f\"‚úÖ API Key configurada: {api_key[:7]}...\")\n",
    "\n",
    "# Inicializar tracker de m√©tricas\n",
    "metrics = MetricsTracker()\n",
    "config = RAGMasterConfig()\n",
    "\n",
    "print(\"\\nüöÄ Ambiente listo para M√≥dulo 1: Fundamentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Los 4 Pilares de RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2: Visualizar arquitectura RAG\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Componentes\n",
    "components = [\n",
    "    {\"name\": \"üìÑ Documentos\\nOriginales\", \"pos\": (1, 6), \"color\": \"lightblue\"},\n",
    "    {\"name\": \"‚úÇÔ∏è Text\\nSplitter\", \"pos\": (3, 6), \"color\": \"lightgreen\"},\n",
    "    {\"name\": \"üî¢ Embeddings\\nModel\", \"pos\": (5, 6), \"color\": \"lightyellow\"},\n",
    "    {\"name\": \"üíæ Vector\\nDatabase\", \"pos\": (7, 6), \"color\": \"lightcoral\"},\n",
    "    {\"name\": \"‚ùì User\\nQuery\", \"pos\": (1, 3), \"color\": \"lightgray\"},\n",
    "    {\"name\": \"üîç Semantic\\nSearch\", \"pos\": (4, 3), \"color\": \"lightgreen\"},\n",
    "    {\"name\": \"üìë Retrieved\\nContext\", \"pos\": (7, 3), \"color\": \"lightyellow\"},\n",
    "    {\"name\": \"ü§ñ LLM\\n(GPT)\", \"pos\": (4, 0.5), \"color\": \"lightblue\"},\n",
    "    {\"name\": \"üí¨ Final\\nAnswer\", \"pos\": (7, 0.5), \"color\": \"lightgreen\"}\n",
    "]\n",
    "\n",
    "# Dibujar componentes\n",
    "for comp in components:\n",
    "    box = FancyBboxPatch(\n",
    "        (comp[\"pos\"][0]-0.4, comp[\"pos\"][1]-0.3),\n",
    "        0.8, 0.6,\n",
    "        boxstyle=\"round,pad=0.1\",\n",
    "        facecolor=comp[\"color\"],\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=2\n",
    "    )\n",
    "    ax.add_patch(box)\n",
    "    ax.text(comp[\"pos\"][0], comp[\"pos\"][1], comp[\"name\"], \n",
    "            ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# Flechas\n",
    "arrows = [\n",
    "    ((1.4, 6), (2.6, 6), \"Cargar\"),\n",
    "    ((3.4, 6), (4.6, 6), \"Chunks\"),\n",
    "    ((5.4, 6), (6.6, 6), \"Vectors\"),\n",
    "    ((7, 5.7), (7, 3.3), \"Store\"),\n",
    "    ((1.4, 3), (3.6, 3), \"Embed\"),\n",
    "    ((4.4, 3), (6.6, 3), \"Top-K\"),\n",
    "    ((7, 2.7), (4.4, 0.8), \"Context\"),\n",
    "    ((1, 2.7), (3.6, 0.8), \"Query\"),\n",
    "    ((4.4, 0.5), (6.6, 0.5), \"Generate\")\n",
    "]\n",
    "\n",
    "for start, end, label in arrows:\n",
    "    arrow = FancyArrowPatch(\n",
    "        start, end,\n",
    "        arrowstyle=\"->\",\n",
    "        connectionstyle=\"arc3,rad=0.1\",\n",
    "        linewidth=2,\n",
    "        color=\"darkblue\"\n",
    "    )\n",
    "    ax.add_patch(arrow)\n",
    "    \n",
    "    # Label en la flecha\n",
    "    mid_x = (start[0] + end[0]) / 2\n",
    "    mid_y = (start[1] + end[1]) / 2\n",
    "    ax.text(mid_x, mid_y + 0.2, label, fontsize=8, ha=\"center\", style=\"italic\")\n",
    "\n",
    "# Configuraci√≥n del plot\n",
    "ax.set_xlim(0, 8)\n",
    "ax.set_ylim(-0.5, 7)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "# T√≠tulo y fases\n",
    "ax.text(4, 7.5, \"üèóÔ∏è ARQUITECTURA RAG COMPLETA\", fontsize=16, fontweight=\"bold\", ha=\"center\")\n",
    "ax.text(4, 6.8, \"FASE 1: INDEXACI√ìN\", fontsize=10, color=\"blue\", ha=\"center\")\n",
    "ax.text(4, 3.8, \"FASE 2: RETRIEVAL\", fontsize=10, color=\"green\", ha=\"center\")\n",
    "ax.text(4, 1.3, \"FASE 3: GENERACI√ìN\", fontsize=10, color=\"red\", ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Los 3 momentos clave de RAG:\")\n",
    "print(\"1Ô∏è‚É£ INDEXACI√ìN: Preparar y almacenar el conocimiento\")\n",
    "print(\"2Ô∏è‚É£ RETRIEVAL: Encontrar informaci√≥n relevante\")\n",
    "print(\"3Ô∏è‚É£ GENERACI√ìN: Crear respuesta con contexto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Implementaci√≥n B√°sica [08:35-08:55]\n",
    "\n",
    "### üõ†Ô∏è Construyendo tu Primer RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3: Implementaci√≥n del RAG m√°s simple posible\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"Tu primer RAG en 50 l√≠neas de c√≥digo\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"üöÄ Inicializando SimpleRAG...\")\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.chroma = chromadb.Client()\n",
    "        \n",
    "        # Crear colecci√≥n (como una tabla en base de datos)\n",
    "        try:\n",
    "            self.collection = self.chroma.create_collection(\"simple_rag\")\n",
    "            print(\"‚úÖ Colecci√≥n creada\")\n",
    "        except:\n",
    "            self.chroma.delete_collection(\"simple_rag\")\n",
    "            self.collection = self.chroma.create_collection(\"simple_rag\")\n",
    "            print(\"‚ôªÔ∏è Colecci√≥n recreada\")\n",
    "    \n",
    "    def load_document(self, filepath: str) -> str:\n",
    "        \"\"\"Cargar documento (PDF o TXT)\"\"\"\n",
    "        print(f\"üìÑ Cargando: {filepath}\")\n",
    "        \n",
    "        if filepath.endswith('.pdf'):\n",
    "            with open(filepath, 'rb') as file:\n",
    "                pdf = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    text += page.extract_text()\n",
    "        else:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "        \n",
    "        print(f\"‚úÖ Documento cargado: {len(text)} caracteres\")\n",
    "        return text\n",
    "    \n",
    "    def create_chunks(self, text: str, chunk_size: int = 500) -> List[str]:\n",
    "        \"\"\"Dividir texto en chunks\"\"\"\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size):\n",
    "            chunk = text[i:i+chunk_size]\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        print(f\"‚úÇÔ∏è Creados {len(chunks)} chunks de ~{chunk_size} caracteres\")\n",
    "        return chunks\n",
    "    \n",
    "    def index_chunks(self, chunks: List[str]):\n",
    "        \"\"\"Indexar chunks en vector database\"\"\"\n",
    "        print(f\"üî¢ Indexando {len(chunks)} chunks...\")\n",
    "        \n",
    "        # A√±adir a ChromaDB (√©l se encarga de los embeddings)\n",
    "        self.collection.add(\n",
    "            documents=chunks,\n",
    "            ids=[f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Chunks indexados en ChromaDB\")\n",
    "    \n",
    "    def query(self, question: str, k: int = 3) -> str:\n",
    "        \"\"\"Hacer una pregunta al RAG\"\"\"\n",
    "        print(f\"\\n‚ùì Pregunta: {question}\")\n",
    "        \n",
    "        # 1. Buscar chunks relevantes\n",
    "        start_time = time.time()\n",
    "        results = self.collection.query(\n",
    "            query_texts=[question],\n",
    "            n_results=k\n",
    "        )\n",
    "        retrieval_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # 2. Preparar contexto\n",
    "        context = \"\\n\\n\".join(results['documents'][0])\n",
    "        print(f\"üîç Recuperados {k} chunks relevantes en {retrieval_time:.0f}ms\")\n",
    "        \n",
    "        # 3. Generar respuesta con LLM\n",
    "        prompt = f\"\"\"\n",
    "        Contexto:\n",
    "        {context}\n",
    "        \n",
    "        Pregunta: {question}\n",
    "        \n",
    "        Instrucciones: Responde bas√°ndote √öNICAMENTE en el contexto proporcionado.\n",
    "        Si no encuentras la informaci√≥n en el contexto, di \"No tengo esa informaci√≥n\".\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        generation_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        total_time = retrieval_time + generation_time\n",
    "        \n",
    "        # M√©tricas\n",
    "        print(f\"‚è±Ô∏è Tiempos: Retrieval={retrieval_time:.0f}ms, Generation={generation_time:.0f}ms, Total={total_time:.0f}ms\")\n",
    "        \n",
    "        # Registrar en m√©tricas globales\n",
    "        cost = len(prompt) / 1000 * 0.0015 + len(answer) / 1000 * 0.002  # Aproximado\n",
    "        metrics.log_query(\n",
    "            module=Module.BASICS,\n",
    "            query=question,\n",
    "            response=answer,\n",
    "            latency=total_time,\n",
    "            cost=cost,\n",
    "            tokens=len(prompt.split()) + len(answer.split())\n",
    "        )\n",
    "        \n",
    "        return answer\n",
    "\n",
    "# Crear instancia\n",
    "rag = SimpleRAG()\n",
    "print(\"\\n‚úÖ ¬°Tu primer RAG est√° listo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4: Cargar y procesar documento\n",
    "# Usamos un documento de ejemplo\n",
    "doc_path = \"../data/company_handbook.pdf\"\n",
    "\n",
    "# Verificar que existe\n",
    "if not Path(doc_path).exists():\n",
    "    # Crear documento de ejemplo si no existe\n",
    "    print(\"üìù Creando documento de ejemplo...\")\n",
    "    !python ../src/utils.py --create-sample-data\n",
    "\n",
    "# Cargar documento\n",
    "text = rag.load_document(doc_path)\n",
    "\n",
    "# Ver preview\n",
    "print(\"\\nüìñ Preview del documento:\")\n",
    "print(\"-\" * 50)\n",
    "print(text[:500] + \"...\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5: Chunking e Indexaci√≥n\n",
    "print(\"üî® PASO 1: CHUNKING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear chunks\n",
    "chunks = rag.create_chunks(text, chunk_size=500)\n",
    "\n",
    "# Mostrar estad√≠sticas\n",
    "print(f\"\\nüìä Estad√≠sticas de chunking:\")\n",
    "print(f\"- Total chunks: {len(chunks)}\")\n",
    "print(f\"- Tama√±o promedio: {np.mean([len(c) for c in chunks]):.0f} chars\")\n",
    "print(f\"- Chunk m√°s peque√±o: {min([len(c) for c in chunks])} chars\")\n",
    "print(f\"- Chunk m√°s grande: {max([len(c) for c in chunks])} chars\")\n",
    "\n",
    "# Ver algunos chunks\n",
    "print(\"\\nüëÄ Ejemplo de chunks:\")\n",
    "for i in range(min(3, len(chunks))):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(chunks[i][:150] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6: Indexar en Vector Database\n",
    "print(\"üíæ PASO 2: INDEXACI√ìN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Indexar solo los primeros 20 chunks para rapidez\n",
    "# En producci√≥n indexar√≠as todos\n",
    "chunks_to_index = chunks[:20]\n",
    "\n",
    "print(f\"\\nüéØ Indexando {len(chunks_to_index)} chunks en ChromaDB...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rag.index_chunks(chunks_to_index)\n",
    "\n",
    "indexing_time = (time.time() - start_time) * 1000\n",
    "print(f\"‚úÖ Indexaci√≥n completada en {indexing_time:.0f}ms\")\n",
    "print(f\"‚ö° Velocidad: {len(chunks_to_index) / (indexing_time/1000):.1f} chunks/segundo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Pr√°ctica - Tu Turno [08:55-09:30]\n",
    "\n",
    "### üéØ Hora de Experimentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 7: Tu primera query RAG\n",
    "print(\"üöÄ TU PRIMERA QUERY RAG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# La query h√©roe del d√≠a\n",
    "respuesta = rag.query(\"¬øCu√°l es la pol√≠tica de vacaciones?\")\n",
    "\n",
    "print(\"\\nüí¨ Respuesta:\")\n",
    "print(\"-\" * 40)\n",
    "print(respuesta)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Verificar si es correcta\n",
    "evaluation = TestSuite.evaluate_response(respuesta, Module.BASICS)\n",
    "print(f\"\\nüìä Evaluaci√≥n:\")\n",
    "print(f\"- Score: {evaluation['score']:.2f}/1.0\")\n",
    "print(f\"- ¬øPas√≥?: {'‚úÖ S√≠' if evaluation['passed'] else '‚ùå No'}\")\n",
    "print(f\"- Tiene info b√°sica: {'‚úÖ' if evaluation['has_basic_info'] else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8: Experimentar con diferentes queries\n",
    "print(\"üß™ EXPERIMENTO: Diferentes tipos de preguntas\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_queries = [\n",
    "    \"¬øCu√°l es el horario de trabajo?\",\n",
    "    \"¬øHay trabajo remoto?\",\n",
    "    \"¬øCu√°les son los beneficios?\",\n",
    "    \"¬øC√≥mo es el proceso de onboarding?\",\n",
    "    \"¬øQu√© pasa si me caso?\"  # Edge case\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    answer = rag.query(query, k=3)\n",
    "    \n",
    "    print(f\"\\nüí¨ Respuesta:\")\n",
    "    print(answer[:200] + \"...\" if len(answer) > 200 else answer)\n",
    "    \n",
    "    # Pausa para no saturar la API\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 9: Experimentar con par√°metros\n",
    "print(\"‚öôÔ∏è EXPERIMENTO: Ajustando par√°metros\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "query_test = \"¬øCu√°les son todos los beneficios de la empresa?\"\n",
    "\n",
    "# Probar diferentes valores de K (chunks recuperados)\n",
    "k_values = [1, 3, 5, 10]\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nüîç Probando con k={k} chunks...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    answer = rag.query(query_test, k=k)\n",
    "    latency = (time.time() - start) * 1000\n",
    "    \n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'answer_length': len(answer),\n",
    "        'latency_ms': latency,\n",
    "        'answer_preview': answer[:100]\n",
    "    })\n",
    "    \n",
    "    print(f\"- Longitud respuesta: {len(answer)} chars\")\n",
    "    print(f\"- Latencia: {latency:.0f}ms\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Visualizar resultados\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nüìä Resumen de resultados:\")\n",
    "print(df[['k', 'answer_length', 'latency_ms']])\n",
    "\n",
    "# Conclusi√≥n\n",
    "print(\"\\nüí° Observaci√≥n: M√°s chunks = respuestas m√°s completas pero mayor latencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 10: Probar con pregunta que NO est√° en el documento\n",
    "print(\"‚ùå EXPERIMENTO: Pregunta sin respuesta\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pregunta que no deber√≠a estar en el manual\n",
    "pregunta_imposible = \"¬øCu√°l es el precio de las acciones de la empresa en bolsa?\"\n",
    "\n",
    "print(f\"\\n‚ùì Pregunta: {pregunta_imposible}\")\n",
    "respuesta = rag.query(pregunta_imposible)\n",
    "\n",
    "print(f\"\\nüí¨ Respuesta:\")\n",
    "print(\"-\" * 40)\n",
    "print(respuesta)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Verificar si el sistema reconoce que no tiene la informaci√≥n\n",
    "if \"no tengo\" in respuesta.lower() or \"no encuentro\" in respuesta.lower():\n",
    "    print(\"\\n‚úÖ ¬°Bien! El RAG reconoce cuando no tiene informaci√≥n\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Cuidado: El RAG podr√≠a estar alucinando\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä An√°lisis de M√©tricas del M√≥dulo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 11: Ver m√©tricas acumuladas\n",
    "print(\"üìà M√âTRICAS DEL M√ìDULO 1\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Obtener resumen\n",
    "summary = metrics.get_summary()\n",
    "\n",
    "if Module.BASICS.name in summary:\n",
    "    stats = summary[Module.BASICS.name]\n",
    "    \n",
    "    print(f\"\\nüìä Estad√≠sticas:\")\n",
    "    print(f\"- Queries realizadas: {stats['queries_count']}\")\n",
    "    print(f\"- Latencia promedio: {stats['avg_latency_ms']:.0f}ms\")\n",
    "    print(f\"- Costo total: ${stats['total_cost_usd']:.4f}\")\n",
    "    print(f\"- Tokens totales: {stats['total_tokens']}\")\n",
    "    \n",
    "    # Comparar con target\n",
    "    target = config.target_metrics[Module.BASICS]\n",
    "    print(f\"\\nüéØ Comparaci√≥n con objetivos:\")\n",
    "    print(f\"- Latencia: {stats['avg_latency_ms']:.0f}ms vs objetivo {target['latency']}ms\")\n",
    "    print(f\"- Costo promedio: ${stats['total_cost_usd']/stats['queries_count']:.4f} vs objetivo ${target['cost']}\")\n",
    "    \n",
    "    # Visualizar\n",
    "    metrics.plot_progress()\n",
    "else:\n",
    "    print(\"No hay m√©tricas registradas a√∫n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Desaf√≠os Adicionales (Si terminas antes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESAF√çO 1: Mejorar el chunking\n",
    "def smart_chunking(text: str, chunk_size: int = 500) -> List[str]:\n",
    "    \"\"\"\n",
    "    TODO: Implementa un chunking m√°s inteligente que:\n",
    "    - No corte frases a la mitad\n",
    "    - Respete p√°rrafos cuando sea posible\n",
    "    - Mantenga contexto entre chunks\n",
    "    \"\"\"\n",
    "    # Tu c√≥digo aqu√≠\n",
    "    pass\n",
    "\n",
    "# DESAF√çO 2: A√±adir metadatos\n",
    "def index_with_metadata(chunks: List[str], metadata: List[Dict]):\n",
    "    \"\"\"\n",
    "    TODO: Indexa chunks con metadata adicional como:\n",
    "    - N√∫mero de p√°gina\n",
    "    - Secci√≥n del documento\n",
    "    - Fecha de actualizaci√≥n\n",
    "    \"\"\"\n",
    "    # Tu c√≥digo aqu√≠\n",
    "    pass\n",
    "\n",
    "# DESAF√çO 3: Implementar re-ranking\n",
    "def rerank_results(query: str, results: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    TODO: Re-ordenar resultados bas√°ndose en:\n",
    "    - Relevancia sem√°ntica\n",
    "    - Longitud del chunk\n",
    "    - Presencia de keywords\n",
    "    \"\"\"\n",
    "    # Tu c√≥digo aqu√≠\n",
    "    pass\n",
    "\n",
    "print(\"üí™ ¬°Intenta resolver estos desaf√≠os!\")\n",
    "print(\"Pista: Mira el m√≥dulo 2 para inspiraci√≥n üòâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ ¬°Felicitaciones!\n",
    "\n",
    "### ‚úÖ Lo que has logrado en el M√≥dulo 1:\n",
    "\n",
    "1. **Construiste** tu primer sistema RAG funcional\n",
    "2. **Indexaste** documentos en un vector database\n",
    "3. **Realizaste** b√∫squeda sem√°ntica\n",
    "4. **Generaste** respuestas con contexto\n",
    "5. **Mediste** latencia y costos\n",
    "\n",
    "### üìä Tus m√©tricas actuales:\n",
    "- ‚è±Ô∏è Latencia: ~2000ms\n",
    "- üí∞ Costo: ~$0.01 por query\n",
    "- üéØ Accuracy: ~70%\n",
    "\n",
    "### üöÄ En el M√≥dulo 2 mejoraremos:\n",
    "- Reducir latencia a 1000ms (-50%)\n",
    "- Reducir costos a $0.008 (-20%)\n",
    "- Aumentar accuracy a 80% (+10%)\n",
    "\n",
    "---\n",
    "\n",
    "**‚òï Toma un break de 15 minutos y nos vemos en el M√≥dulo 2!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}