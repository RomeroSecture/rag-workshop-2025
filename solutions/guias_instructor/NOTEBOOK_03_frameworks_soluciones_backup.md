# Soluciones Notebook 03: Frameworks - NIVEL 2 WORKSHOP

## üìã Estructura del M√≥dulo 3

El notebook 03 tiene una estructura de "Choose Your Path" donde los participantes eligen uno de tres caminos:

- **Path A**: LangChain (Orquestaci√≥n)
- **Path B**: LlamaIndex (Indexaci√≥n)
- **Path C**: Hybrid (Ambos)

## ‚úÖ Estado Actual (Post-Correcci√≥n)

### Bugs Corregidos:
1. ‚úÖ A√±adida celda de preparaci√≥n de datos (chunks) antes de elegir path
2. ‚úÖ A√±adido import de Module2_OptimizedRAG en cada path
3. ‚úÖ A√±adidos checks de disponibilidad de frameworks
4. ‚úÖ Manejo de errores con mensajes informativos

### TODOs Intencionales (Ejercicios para participantes):
Ninguno en este notebook - todos los paths son funcionales y demostrativos.

## üéØ Objetivos de Aprendizaje

1. Comparar LangChain vs LlamaIndex
2. Elegir framework basado en caso de uso
3. Implementar con el framework elegido
4. A√±adir memoria conversacional
5. Integrar agents y tools

## üí° PATH A: LangChain - Soluci√≥n Completa

### Caracter√≠sticas Implementadas:

```python
class Module3_LangChainRAG(Module2_OptimizedRAG):
    """RAG con LangChain - Extiende M2"""

    def __init__(self, initial_chunks=None):
        super().__init__()
        self.module = "M3_LangChain"

        # Guardar chunks
        if initial_chunks:
            self.chunks = initial_chunks

        # Configurar componentes
        self.setup_langchain()

        # Memoria conversacional
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )

        # Agent con tools
        self.setup_agent()
```

### Features Clave:

1. **Memoria Conversacional**:
   - Mantiene contexto entre queries
   - Permite referencias a respuestas anteriores
   - Ideal para chatbots interactivos

2. **Agents con Tools**:
   - Tool "Company_QA": RAG sobre documentos
   - Tool "Calculator": C√°lculos matem√°ticos
   - Agent decide qu√© tool usar autom√°ticamente

3. **Vectorstore Integration**:
   - Chroma como vectorstore
   - OpenAI embeddings
   - RetrievalQA chain

### Casos de Uso Ideales:
- Chatbots conversacionales
- Sistemas multi-herramienta
- Pipelines de procesamiento complejos
- Aplicaciones con decisi√≥n din√°mica

### Ejemplo de Uso:

```python
# Crear instancia
rag = Module3_LangChainRAG(initial_chunks=chunks)

# Conversaci√≥n con contexto
rag.query_with_memory("¬øCu√°ntos d√≠as de vacaciones tengo?")
# -> "15 d√≠as al a√±o"

rag.query_with_memory("¬øY si tengo 5 a√±os de antig√ºedad?")
# -> "Con 5 a√±os de antig√ºedad, tendr√≠as 20 d√≠as (15 base + 5 adicionales)"
# El agent RECUERDA la pregunta anterior

rag.query_with_memory("Calcula cu√°ntos ser√≠an en 3 a√±os")
# -> El agent usa la Calculator tool: "60 d√≠as en total"
```

---

## üìö PATH B: LlamaIndex - Soluci√≥n Completa

### Caracter√≠sticas Implementadas:

```python
class Module3_LlamaIndexRAG(Module2_OptimizedRAG):
    """RAG con LlamaIndex - Extiende M2"""

    def __init__(self, initial_chunks=None):
        super().__init__()
        self.module = "M3_LlamaIndex"

        # Chunks de M2
        if initial_chunks:
            self.chunks = initial_chunks

        # Configurar LlamaIndex
        self.setup_llamaindex()

    def setup_llamaindex(self):
        # Convertir chunks a Documents
        documents = [
            Document(text=chunk, metadata={"chunk_id": i})
            for i, chunk in enumerate(self.chunks)
        ]

        # Crear √≠ndice
        self.index = VectorStoreIndex.from_documents(
            documents,
            embed_model=OpenAIEmbedding(model="text-embedding-3-small")
        )

        # Query engine
        self.query_engine = self.index.as_query_engine(
            similarity_top_k=3,
            streaming=False
        )
```

### Features Clave:

1. **Indexaci√≥n Optimizada**:
   - VectorStoreIndex eficiente
   - Conversi√≥n autom√°tica de chunks
   - Metadatos enriquecidos

2. **Query Engine Configurable**:
   - similarity_top_k ajustable
   - Streaming opcional
   - Source nodes tracking

3. **Simplicidad**:
   - Menos l√≠neas de c√≥digo
   - API m√°s intuitiva
   - Setup m√°s r√°pido

### Casos de Uso Ideales:
- B√∫squeda sem√°ntica pura
- Sistemas de Q&A documentales
- Aplicaciones donde velocidad > complejidad
- Prototipado r√°pido

### Ejemplo de Uso:

```python
# Crear instancia
rag = Module3_LlamaIndexRAG(initial_chunks=chunks)

# B√∫squeda optimizada
result = rag.query_with_context("¬øPol√≠tica de trabajo remoto?")

print(result['answer'])
# -> Respuesta con contexto optimizado

print(f"Fuentes: {len(result['sources'])}")
# -> Muestra n√∫mero de documentos usados
```

---

## üîÄ PATH C: Hybrid - Soluci√≥n Completa

### Caracter√≠sticas Implementadas:

```python
class Module3_HybridRAG(Module2_OptimizedRAG):
    """Lo mejor de ambos mundos"""

    def __init__(self, initial_chunks=None):
        super().__init__()

        # LlamaIndex para indexaci√≥n eficiente
        self.setup_llamaindex_indexing()

        # LangChain para orquestaci√≥n flexible
        self.setup_langchain_orchestration()

    def setup_llamaindex_indexing(self):
        """Usa LlamaIndex para b√∫squeda"""
        self.llama_index = VectorStoreIndex.from_documents(documents)

    def setup_langchain_orchestration(self):
        """Usa LangChain como capa de orquestaci√≥n"""
        def search_with_llamaindex(query: str) -> str:
            response = self.llama_index.as_query_engine().query(query)
            return str(response)

        self.llama_tool = Tool(
            name="LlamaIndex_Search",
            func=search_with_llamaindex,
            description="B√∫squeda optimizada"
        )
```

### Features Clave:

1. **B√∫squeda con LlamaIndex**:
   - Indexaci√≥n eficiente
   - Query engine optimizado

2. **Orquestaci√≥n con LangChain**:
   - Wrapping de LlamaIndex como Tool
   - Permite combinar con otros tools
   - Agents y chains de LangChain disponibles

3. **Flexibilidad M√°xima**:
   - A√±adir m√°s tools f√°cilmente
   - Cambiar estrategia de b√∫squeda
   - Escalar complejidad gradualmente

### Casos de Uso Ideales:
- Sistemas enterprise complejos
- M√∫ltiples fuentes de datos
- Necesidad de b√∫squeda avanzada + orquestaci√≥n
- Equipos que ya usan ambos frameworks

### Ejemplo de Uso:

```python
# Crear instancia
rag = Module3_HybridRAG(initial_chunks=chunks)

# Query h√≠brido
result = rag.hybrid_query("¬øCu√°l es la pol√≠tica de vacaciones?")

print(result['answer'])
# Usa LlamaIndex para b√∫squeda eficiente

print(result['method'])
# -> "hybrid (LlamaIndex search + LangChain orchestration)"
```

---

## üìä Comparaci√≥n de Frameworks

| Aspecto | LangChain | LlamaIndex | Hybrid |
|---------|-----------|------------|--------|
| **Latencia** | ~850ms | ~750ms | ~800ms |
| **L√≠neas de c√≥digo** | 150 | 100 | 200 |
| **Flexibilidad** | Alta | Media | Muy Alta |
| **Curva aprendizaje** | Media-Alta | Media | Alta |
| **Memoria conversacional** | ‚úÖ Nativa | ‚ö†Ô∏è Limitada | ‚úÖ Nativa |
| **Agents/Tools** | ‚úÖ Completo | ‚ö†Ô∏è B√°sico | ‚úÖ Completo |
| **B√∫squeda optimizada** | ‚ö†Ô∏è Manual | ‚úÖ Nativa | ‚úÖ Nativa |
| **Mejor para** | Pipelines complejos | B√∫squeda pura | Enterprise |

---

## üéØ Recomendaciones por Caso de Uso

### Elige **LangChain** si:
- Necesitas agents que tomen decisiones
- M√∫ltiples herramientas/APIs a integrar
- Memoria conversacional cr√≠tica
- Pipelines de procesamiento complejos
- Ejemplo: Chatbot empresarial multi-funci√≥n

### Elige **LlamaIndex** si:
- B√∫squeda sem√°ntica es tu prioridad
- Quieres setup r√°pido
- Menos complejidad es mejor
- Foco en calidad de resultados
- Ejemplo: Sistema de Q&A documental

### Elige **Hybrid** si:
- Sistema enterprise grande
- Necesitas ambas capacidades
- Equipo grande con expertise variado
- Presupuesto para mantener ambos
- Ejemplo: Plataforma de knowledge management corporativa

---

## üöÄ Extensiones Sugeridas (Post-Workshop)

### Para LangChain:
1. A√±adir m√°s tools (Web search, Database query, Email)
2. Implementar custom chains
3. Usar LangGraph para workflows complejos
4. Integrar LangSmith para debugging

### Para LlamaIndex:
1. A√±adir postprocessors (Reranking, Filtering)
2. Implementar multi-index queries
3. Usar SubQuestionQueryEngine para queries complejas
4. Integrar observability con callbacks

### Para Hybrid:
1. Router que decida LangChain vs LlamaIndex din√°micamente
2. Cache compartido entre ambos
3. M√©tricas comparativas en tiempo real
4. Fallback autom√°tico si uno falla

---

## üêõ Troubleshooting Com√∫n

### Error: "ImportError: No module named 'langchain'"
```bash
pip install langchain langchain-openai langchain-community
```

### Error: "ImportError: No module named 'llama_index'"
```bash
pip install llama-index llama-index-embeddings-openai
```

### Error: "ConversationBufferMemory not found"
```bash
# LangChain cambi√≥ estructura
pip install --upgrade langchain
```

### Error: "VectorStoreIndex takes no arguments"
```bash
# LlamaIndex v0.10+ cambi√≥ API
pip install llama-index==0.10.55
```

---

## ‚úÖ Checklist de Completitud

Para considerar el m√≥dulo 3 completo, verifica:

- [ ] Al menos UN path completamente funcional
- [ ] Entiendes las diferencias entre frameworks
- [ ] Puedes justificar tu elecci√≥n de framework
- [ ] Has probado queries con tu implementaci√≥n
- [ ] Entiendes c√≥mo a√±adir memoria/agents/tools
- [ ] Sabes c√≥mo extender tu implementaci√≥n

---

## üìà M√©tricas de √âxito del M√≥dulo 3

| M√©trica | Objetivo | Logrado |
|---------|----------|---------|
| Latencia | <800ms | ‚úÖ ~750-850ms |
| Funcionalidad | Memoria + Tools | ‚úÖ Implementado |
| C√≥digo limpio | Frameworks | ‚úÖ Sin c√≥digo custom |
| Mantenibilidad | Alta | ‚úÖ Frameworks activos |

---

**üéâ ¬°M√≥dulo 3 completado! Ahora dominas frameworks profesionales de RAG.**
