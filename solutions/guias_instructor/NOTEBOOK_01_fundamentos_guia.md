# üéì Gu√≠a del Instructor - Notebook 01: Fundamentos de RAG
## De Cero a tu Primer Sistema RAG Funcional

---

## üìã Informaci√≥n General

**Duraci√≥n:** 75 minutos (08:15-09:30)
**Objetivo:** Construir un sistema RAG funcional desde cero
**Nivel:** Fundamentos
**Pre-requisitos:** Haber completado Notebook 00 (setup verificado)

---

## üéØ Objetivos de Aprendizaje

Al finalizar este m√≥dulo, los participantes ser√°n capaces de:

1. ‚úÖ **Explicar** qu√© es RAG y sus 3 fases (Indexaci√≥n, Retrieval, Generaci√≥n)
2. ‚úÖ **Implementar** un pipeline RAG completo en 50 l√≠neas de c√≥digo
3. ‚úÖ **Cargar** documentos (PDF/TXT) y procesarlos
4. ‚úÖ **Crear** chunks de texto efectivos
5. ‚úÖ **Indexar** en una vector database (ChromaDB)
6. ‚úÖ **Realizar** b√∫squeda sem√°ntica
7. ‚úÖ **Generar** respuestas con contexto usando LLM
8. ‚úÖ **Medir** latencia, costo y calidad

---

## üóìÔ∏è Timeline Detallado

| Tiempo | Secci√≥n | Actividad | Celdas |
|--------|---------|-----------|--------|
| 00:00-00:20 | Parte 1: Teor√≠a | Conceptos y arquitectura RAG | 1-4 |
| 00:20-00:40 | Parte 2: Implementaci√≥n | Construir SimpleRAG | 5-9 |
| 00:40-01:15 | Parte 3: Pr√°ctica | Experimentos y optimizaci√≥n | 10-18 |

---

## üìù Gui√≥n de la Sesi√≥n

### PARTE 1: Setup y Conceptos [08:15-08:35] - 20 min

#### 1. Introducci√≥n a RAG (5 min)

**Instructor presenta (Celda 1):**

> "RAG significa **Retrieval-Augmented Generation**. Es una t√©cnica que combina:
> - üìö Una base de conocimiento (tus documentos)
> - üîç Un buscador ultra-r√°pido (vector search)
> - ü§ñ Un LLM (como GPT) para generar respuestas
>
> Es como darle a ChatGPT acceso a Google, pero solo sobre TUS documentos."

**Conceptos clave a enfatizar:**
- **Retrieval** = Buscar informaci√≥n relevante
- **Augmented** = Enriquecer el contexto del LLM
- **Generation** = Crear respuesta basada en contexto

**Ejecutar Celda 2:**
```python
import sys, os
from pathlib import Path
sys.path.append(str(Path.cwd().parent / 'src'))

from module_1_basics import Module1_BasicRAG
from shared_config import RAGMasterConfig, TestSuite, MetricsTracker, Module
```

**Salida esperada:**
```
‚úÖ API Key configurada: sk-proj...
üöÄ Ambiente listo para M√≥dulo 1: Fundamentos
```

#### 2. Los 4 Pilares de RAG (7 min)

**Instructor ejecuta y explica Celda 4 (diagrama arquitectura):**

**El diagrama muestra 3 fases:**

**FASE 1: INDEXACI√ìN** (preparar conocimiento)
```
üìÑ Documentos ‚Üí ‚úÇÔ∏è Text Splitter ‚Üí üî¢ Embeddings ‚Üí üíæ Vector DB
```

**Explicaci√≥n paso a paso:**
- **Documentos Originales:** PDFs, TXTs, JSONs con tu conocimiento
- **Text Splitter:** Divide en chunks manejables (500-1000 chars)
- **Embeddings Model:** Convierte texto a vectores num√©ricos (arrays de 1536 dimensiones)
- **Vector Database:** Almacena vectores para b√∫squeda r√°pida

**FASE 2: RETRIEVAL** (buscar relevante)
```
‚ùì User Query ‚Üí üî¢ Embed Query ‚Üí üîç Semantic Search ‚Üí üìë Top-K Results
```

**Explicaci√≥n:**
- **User Query:** "¬øCu√°l es la pol√≠tica de vacaciones?"
- **Embed Query:** Convertir pregunta a vector (mismo espacio que los documentos)
- **Semantic Search:** Calcular similitud coseno entre query y todos los chunks
- **Top-K Results:** Devolver los K chunks m√°s similares (t√≠picamente K=3-5)

**FASE 3: GENERACI√ìN** (crear respuesta)
```
üìë Retrieved Context + ‚ùì Query ‚Üí ü§ñ LLM ‚Üí üí¨ Final Answer
```

**Explicaci√≥n:**
- **Prompt Engineering:** Combinar contexto + pregunta en un prompt efectivo
- **LLM Generation:** GPT genera respuesta bas√°ndose SOLO en el contexto
- **Final Answer:** Respuesta fundamentada en tus documentos

**üí° Analog√≠a √∫til:**
> "Es como buscar en un libro:
> 1. **Indexaci√≥n** = Crear un √≠ndice del libro
> 2. **Retrieval** = Buscar en el √≠ndice las p√°ginas relevantes
> 3. **Generaci√≥n** = Leer esas p√°ginas y resumir la respuesta"

#### 3. M√©tricas Clave (3 min)

**Instructor enfatiza las 3 m√©tricas principales:**

| M√©trica | Qu√© mide | Objetivo M√≥dulo 1 |
|---------|----------|-------------------|
| **Latencia** | Tiempo de respuesta | < 2000ms |
| **Costo** | $ por query | < $0.01 |
| **Accuracy** | Calidad de respuesta | > 70% |

**Explicaci√≥n:**
- **Latencia:** Suma de tiempo de retrieval (~500ms) + generaci√≥n (~1500ms)
- **Costo:** Basado en tokens de entrada (context + query) y salida (respuesta)
- **Accuracy:** Evaluada con TestSuite.evaluate_response()

**üí° Insight:**
> "En producci√≥n, necesitas balance. Una respuesta perfecta (100% accuracy) que tarde 30 segundos no sirve. Buscaremos 90% accuracy en <1 segundo."

#### 4. Preguntas y Aclaraciones (5 min)

**Preguntas frecuentes esperadas:**

**P: "¬øPor qu√© no simplemente pasarle todo el documento a GPT?"**
R: "GPT-3.5 tiene l√≠mite de ~4K tokens de contexto. Un documento t√≠pico tiene 50K+ tokens. Adem√°s, cuesta m√°s y tarda m√°s."

**P: "¬øLos embeddings son caros de generar?"**
R: "No, text-embedding-3-small cuesta $0.00002 por 1K tokens. Indexar 100 p√°ginas cuesta ~$0.20 total."

**P: "¬øQu√© tan buena es la b√∫squeda sem√°ntica?"**
R: "Muy buena. Encuentra 'd√≠as de descanso' cuando preguntas por 'vacaciones' porque entiende el significado, no solo keywords."

---

### PARTE 2: Implementaci√≥n B√°sica [08:35-08:55] - 20 min

#### 5. Construyendo SimpleRAG (8 min)

**Instructor ejecuta Celda 6 explicando cada m√©todo:**

```python
class SimpleRAG:
    """Tu primer RAG en 50 l√≠neas de c√≥digo"""
```

**M√©todo 1: __init__**
```python
def __init__(self):
    self.client = OpenAI(api_key=api_key)
    self.chroma = chromadb.Client()
    self.collection = self.chroma.create_collection("simple_rag")
```

**Explicar:**
- ChromaDB usa SQLite bajo el cap√≥ (no requiere servidor)
- La colecci√≥n es como una tabla en SQL
- Si ya existe, se borra y recrea (modo desarrollo)

**M√©todo 2: load_document**
```python
def load_document(self, filepath: str) -> str:
    # Soporta PDF y TXT
    if filepath.endswith('.pdf'):
        pdf = PyPDF2.PdfReader(file)
        text = "".join(page.extract_text() for page in pdf.pages)
```

**Explicar:**
- PyPDF2 extrae texto plano (sin im√°genes/tablas)
- Para producci√≥n considerar: pdfplumber, pypdf, unstructured

**M√©todo 3: create_chunks**
```python
def create_chunks(self, text: str, chunk_size: int = 500) -> List[str]:
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunk = text[i:i+chunk_size]
        chunks.append(chunk)
```

**‚ö†Ô∏è Advertir limitaciones:**
- Este chunking es ingenuo (corta palabras/frases)
- En M√≥dulo 2 mejoraremos con overlap y smart splitting
- Por ahora es suficiente para aprender el concepto

**M√©todo 4: index_chunks**
```python
def index_chunks(self, chunks: List[str]):
    self.collection.add(
        documents=chunks,
        ids=[f"chunk_{i}" for i in range(len(chunks))]
    )
```

**Explicar la "magia" de ChromaDB:**
- ChromaDB genera embeddings autom√°ticamente
- Usa el modelo por defecto de Sentence Transformers
- No necesitas llamar a OpenAI embeddings (ahorra $)

**M√©todo 5: query** (el m√°s importante)
```python
def query(self, question: str, k: int = 3) -> str:
    # 1. Buscar chunks relevantes
    results = self.collection.query(
        query_texts=[question],
        n_results=k
    )

    # 2. Preparar contexto
    context = "\n\n".join(results['documents'][0])

    # 3. Generar respuesta
    prompt = f"""
    Contexto: {context}
    Pregunta: {question}
    Responde bas√°ndote √öNICAMENTE en el contexto.
    """

    response = self.client.chat.completions.create(...)
```

**Explicar cada paso:**
1. **Retrieval:** ChromaDB hace vector search autom√°ticamente
2. **Context Building:** Unimos los K chunks con saltos de l√≠nea
3. **Generation:** Prompt simple pero efectivo

**M√©tricas registradas:**
- Latencia de retrieval
- Latencia de generaci√≥n
- Tokens usados
- Costo estimado

#### 6. Cargar Documento (4 min)

**Instructor ejecuta Celda 7:**

```python
doc_path = "../data/company_handbook.pdf"
text = rag.load_document(doc_path)
```

**Mostrar preview:**
```
üìÑ Cargando: ../data/company_handbook.pdf
‚úÖ Documento cargado: 45,328 caracteres

üìñ Preview del documento:
--------------------------------------------------
MANUAL DEL EMPLEADO

Bienvenido a TechCorp Inc.

Este manual contiene informaci√≥n importante sobre pol√≠ticas...
```

**Explicar el contenido del documento:**
- Manual de empleado ficticio
- Contiene: pol√≠ticas de vacaciones, beneficios, c√≥digo de conducta, etc.
- Perfecto para queries tipo HR/RH

#### 7. Chunking e Indexaci√≥n (4 min)

**Instructor ejecuta Celda 8:**

```python
chunks = rag.create_chunks(text, chunk_size=500)
```

**Salida t√≠pica:**
```
‚úÇÔ∏è Creados 91 chunks de ~500 caracteres

üìä Estad√≠sticas de chunking:
- Total chunks: 91
- Tama√±o promedio: 498 chars
- Chunk m√°s peque√±o: 187 chars  # El √∫ltimo
- Chunk m√°s grande: 500 chars
```

**Mostrar ejemplos de chunks (importante para debugging):**
```
Chunk 1:
----------------------------------------
MANUAL DEL EMPLEADO

Bienvenido a TechCorp Inc.

Este manual contiene informaci√≥n importante sobre pol√≠ticas, beneficios y...

Chunk 2:
----------------------------------------
...y procedimientos de la empresa. Por favor, l√©elo cuidadosamente.

SECCI√ìN 1: POL√çTICAS DE TRABAJO

1.1 Horario Laboral...
```

**‚ö†Ô∏è Se√±alar el problema:**
> "Vean que el Chunk 2 empieza con '...y procedimientos'. Esto pasa porque cortamos en mitad de una frase. Es un problema que resolveremos en M√≥dulo 2 con overlap y smart chunking."

**Ejecutar Celda 9 (indexaci√≥n):**

```python
chunks_to_index = chunks[:20]  # Solo primeros 20 para rapidez
rag.index_chunks(chunks_to_index)
```

**Salida:**
```
üíæ PASO 2: INDEXACI√ìN
==================================================
üéØ Indexando 20 chunks en ChromaDB...
‚úÖ Indexaci√≥n completada en 1,234ms
‚ö° Velocidad: 16.2 chunks/segundo
```

**Explicar por qu√© solo 20 chunks:**
- Para que el workshop sea √°gil
- En producci√≥n indexar√≠as todos
- ChromaDB puede manejar millones de chunks

#### 8. Demo R√°pida (4 min)

**Instructor hace una query en vivo:**

```python
# Query r√°pida para demostrar funcionamiento
respuesta = rag.query("¬øCu√°l es el horario de trabajo?")
```

**Mostrar el output completo:**
```
‚ùì Pregunta: ¬øCu√°l es el horario de trabajo?
üîç Recuperados 3 chunks relevantes en 245ms
‚è±Ô∏è Tiempos: Retrieval=245ms, Generation=1,567ms, Total=1,812ms

üí¨ Respuesta:
Seg√∫n el manual, el horario de trabajo est√°ndar es de 9:00 AM a 6:00 PM,
de lunes a viernes, con una hora de almuerzo.
```

**Explicar el flujo:**
1. Query convertida a embedding (autom√°tico por ChromaDB)
2. B√∫squeda de 3 chunks m√°s similares (245ms)
3. Chunks enviados a GPT como contexto
4. GPT genera respuesta basada en contexto (1,567ms)

---

### PARTE 3: Pr√°ctica - Tu Turno [08:55-09:30] - 35 min

#### 9. Primera Query Guiada (5 min)

**Instructor ejecuta Celda 11 paso a paso:**

```python
respuesta = rag.query("¬øCu√°l es la pol√≠tica de vacaciones?")
```

**Evaluaci√≥n autom√°tica:**
```python
evaluation = TestSuite.evaluate_response(respuesta, Module.BASICS)
```

**Salida esperada:**
```
üìä Evaluaci√≥n:
- Score: 0.75/1.0
- ¬øPas√≥?: ‚úÖ S√≠
- Tiene info b√°sica: ‚úÖ
```

**Explicar c√≥mo funciona la evaluaci√≥n:**
- Verifica si contiene keywords esperadas (ej: "d√≠as", "vacaciones", "22")
- Verifica longitud m√≠nima (>50 chars)
- Verifica que no dice "no s√©" o "no tengo informaci√≥n"

**Si score < 0.7:**
- Puede ser que los chunks relevantes no est√©n en los 20 indexados
- Re-indexar m√°s chunks: `rag.index_chunks(chunks[:50])`

#### 10. Experimentar con Diferentes Queries (8 min)

**Instructor ejecuta Celda 12:**

**Queries de prueba:**
1. "¬øCu√°l es el horario de trabajo?" ‚Üí Deber√≠a responder bien
2. "¬øHay trabajo remoto?" ‚Üí Deber√≠a responder bien
3. "¬øCu√°les son los beneficios?" ‚Üí Deber√≠a listar varios
4. "¬øC√≥mo es el proceso de onboarding?" ‚Üí Puede ser limitado si no indexamos esa parte
5. "¬øQu√© pasa si me caso?" ‚Üí Edge case interesante

**Para cada query mostrar:**
- La respuesta generada
- El tiempo de ejecuci√≥n
- Brevemente analizar si es correcta

**üí° Insight para compartir:**
> "Vean que queries 1-3 funcionan bien porque probablemente est√°n en los primeros 20 chunks. La query 4-5 puede fallar porque esa info est√° m√°s adelante en el documento. Esto demuestra la importancia de indexar TODO el documento."

#### 11. Experimentar con Par√°metros K (10 min)

**Instructor ejecuta Celda 13:**

```python
query_test = "¬øCu√°les son todos los beneficios de la empresa?"
k_values = [1, 3, 5, 10]
```

**Resultados t√≠picos:**

| K | Longitud Respuesta | Latencia | Observaci√≥n |
|---|-------------------|----------|-------------|
| 1 | 120 chars | 1,200ms | Respuesta corta, puede faltar info |
| 3 | 280 chars | 1,800ms | Balance ideal |
| 5 | 450 chars | 2,100ms | M√°s completa pero empieza a ser lenta |
| 10 | 380 chars | 2,800ms | No necesariamente mejor, m√°s lenta |

**Explicar el trade-off:**
- **M√°s chunks (K alto):**
  - ‚úÖ M√°s contexto ‚Üí respuestas m√°s completas
  - ‚ùå M√°s tokens ‚Üí m√°s caro
  - ‚ùå M√°s tiempo de procesamiento ‚Üí m√°s lento
  - ‚ùå Potencial "ruido" (chunks irrelevantes)

- **Menos chunks (K bajo):**
  - ‚úÖ M√°s r√°pido
  - ‚úÖ M√°s barato
  - ‚ùå Puede faltar informaci√≥n

**Conclusi√≥n:**
> "K=3 suele ser el sweet spot. Suficiente contexto sin desperdiciar tokens ni tiempo."

#### 12. Pregunta Sin Respuesta (4 min)

**Instructor ejecuta Celda 14:**

```python
pregunta_imposible = "¬øCu√°l es el precio de las acciones de la empresa en bolsa?"
respuesta = rag.query(pregunta_imposible)
```

**Salida esperada (buena):**
```
üí¨ Respuesta:
No tengo informaci√≥n sobre el precio de las acciones en el manual proporcionado.

‚úÖ ¬°Bien! El RAG reconoce cuando no tiene informaci√≥n
```

**Salida problem√°tica (mala):**
```
üí¨ Respuesta:
El precio actual de las acciones es aproximadamente $45 USD...

‚ö†Ô∏è Cuidado: El RAG podr√≠a estar alucinando
```

**Explicar el concepto de alucinaci√≥n:**
- El LLM tiene conocimiento pre-entrenado
- Puede "inventar" respuestas plausibles si el prompt no es restrictivo
- En M√≥dulo 2 mejoraremos el prompt para prevenir esto

**C√≥mo detectar alucinaciones:**
- Respuestas que no est√°n en el contexto
- Informaci√≥n demasiado espec√≠fica (n√∫meros, fechas) sin source
- Respuestas gen√©ricas que podr√≠an aplicar a cualquier empresa

#### 13. An√°lisis de M√©tricas (5 min)

**Instructor ejecuta Celda 16:**

```python
summary = metrics.get_summary()
stats = summary[Module.BASICS.name]
```

**Salida t√≠pica:**
```
üìà M√âTRICAS DEL M√ìDULO 1
==================================================

üìä Estad√≠sticas:
- Queries realizadas: 8
- Latencia promedio: 1,845ms
- Costo total: $0.0674
- Tokens totales: 3,247

üéØ Comparaci√≥n con objetivos:
- Latencia: 1,845ms vs objetivo 2,000ms ‚úÖ
- Costo promedio: $0.0084 vs objetivo $0.01 ‚úÖ
```

**Explicar las m√©tricas:**
- **Latencia promedio:** ~1,800ms est√° bien para V1 b√°sico
- **Costo por query:** ~$0.008 es razonable con GPT-3.5-turbo
- **Tokens totales:** Acumulado de todas las queries

**Mostrar gr√°fico (si hay tiempo):**
```python
metrics.plot_progress()
```

#### 14. Desaf√≠os Adicionales (3 min - opcional)

**Si los participantes van adelantados, mostrar Celda 18:**

**Desaf√≠o 1: Smart Chunking**
> "Implementa chunking que no corte frases a la mitad. Pista: usa regex para detectar puntos/saltos de l√≠nea."

**Desaf√≠o 2: Metadatos**
> "A√±ade metadatos a cada chunk: n√∫mero de p√°gina, secci√≥n, fecha. √ötil para citar fuentes."

**Desaf√≠o 3: Re-ranking**
> "Implementa un re-ranker que re-ordene resultados por: similitud sem√°ntica + keyword matching + longitud del chunk."

**No dar soluciones ahora:**
> "Estas son ideas avanzadas. En M√≥dulo 2 veremos las soluciones completas. Si quieren intentarlo ahora, adelante!"

---

## üéâ Cierre del M√≥dulo [09:25-09:30] - 5 min

**Instructor ejecuta √∫ltima celda (19) y resume:**

### ‚úÖ Lo que lograron:

1. **Entendieron** las 3 fases de RAG
2. **Construyeron** un sistema RAG funcional en 50 l√≠neas
3. **Indexaron** documentos en vector database
4. **Realizaron** b√∫squeda sem√°ntica
5. **Generaron** respuestas con contexto
6. **Midieron** latencia, costo y calidad

### üìä M√©tricas Actuales:
- ‚è±Ô∏è Latencia: ~1,850ms
- üí∞ Costo: ~$0.008 por query
- üéØ Accuracy: ~75%

### üöÄ En el M√≥dulo 2:
> "Ahora que tienen un RAG funcionando, vamos a **optimizarlo**:
> - Reducir latencia 50% (1,850ms ‚Üí 1,000ms)
> - Implementar caching
> - Chunking inteligente con overlap
> - Re-ranking de resultados
> - Prompts avanzados"

**Transici√≥n:**
> "Tomen un break de 15 minutos. Estiren las piernas, caf√©, y nos vemos en M√≥dulo 2 a las 09:45!"

---

## üìä M√©tricas de √âxito del M√≥dulo

Al final del m√≥dulo, verificar:

- [ ] **90%+** de participantes ejecutaron todas las celdas
- [ ] **80%+** obtuvieron score > 0.70 en evaluaci√≥n
- [ ] **100%** entienden las 3 fases de RAG
- [ ] **90%+** pueden explicar qu√© hace cada m√©todo de SimpleRAG
- [ ] **Latencia promedio** de la clase < 2,500ms
- [ ] **No hay errores** de API key o conexi√≥n

---

## üö® Problemas Comunes y Soluciones

### 1. Error al cargar PDF
**S√≠ntomas:** `PdfReadError` o texto extra√≠do vac√≠o
**Causas:**
- PDF protegido con contrase√±a
- PDF escaneado (imagen, no texto)
**Soluciones:**
```python
# Verificar si el PDF tiene texto
import PyPDF2
with open(doc_path, 'rb') as f:
    pdf = PyPDF2.PdfReader(f)
    print(f"P√°ginas: {len(pdf.pages)}")
    print(f"Texto p√°gina 1: {pdf.pages[0].extract_text()[:200]}")
```

### 2. ChromaDB no crea colecci√≥n
**S√≠ntomas:** `ValueError: Collection already exists`
**Soluci√≥n:**
```python
# Eliminar colecci√≥n existente
try:
    chroma.delete_collection("simple_rag")
except:
    pass
collection = chroma.create_collection("simple_rag")
```

### 3. Query devuelve "No tengo informaci√≥n" para todo
**Causas:**
- No se indexaron chunks (olvidaron ejecutar Celda 9)
- Los chunks indexados no contienen info relevante
**Soluciones:**
```python
# Verificar chunks indexados
print(f"Chunks en DB: {collection.count()}")

# Re-indexar m√°s chunks
rag.index_chunks(chunks[:50])  # Aumentar de 20 a 50
```

### 4. Latencia muy alta (>5 segundos)
**Causas:**
- K muy alto (ej: K=20)
- Modelo GPT-4 en vez de GPT-3.5
- Conexi√≥n lenta
**Soluciones:**
```python
# Reducir K
rag.query(question, k=3)  # En vez de k=10

# Verificar modelo
print(rag.model)  # Debe ser "gpt-3.5-turbo"

# Test de latencia
import time
start = time.time()
response = client.chat.completions.create(...)
print(f"API latency: {(time.time()-start)*1000}ms")
```

### 5. Costos muy altos
**S√≠ntomas:** Warnings de $ gastado
**Causas:**
- Usar GPT-4 en vez de GPT-3.5
- K muy alto con documentos largos
- Temperature alta generando respuestas largas
**Soluciones:**
```python
# Verificar configuraci√≥n
print(f"Model: {rag.model}")  # gpt-3.5-turbo
print(f"Max tokens: {rag.max_tokens}")  # 200
print(f"K: {rag.k}")  # 3

# Limitar tokens de respuesta
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    max_tokens=150,  # Limitar respuesta
    ...
)
```

### 6. Alucinaciones frecuentes
**S√≠ntomas:** Respuestas inventadas no basadas en contexto
**Causas:**
- Prompt permisivo
- Temperature alta
**Soluciones:**
```python
# Prompt m√°s restrictivo
prompt = f"""
Contexto: {context}

Pregunta: {question}

IMPORTANTE: Responde √öNICAMENTE con informaci√≥n del contexto.
Si no encuentras la respuesta en el contexto, di exactamente:
"No tengo esa informaci√≥n en el manual."
"""

# Bajar temperature
response = client.chat.completions.create(
    temperature=0.0,  # M√°s determin√≠stico
    ...
)
```

---

## üí° Tips de Ense√±anza

### Ritmo y Timing
- **Parte 1 (teor√≠a):** No extender m√°s de 20 min
- **Parte 2 (implementaci√≥n):** C√≥digo en vivo, no solo mostrar
- **Parte 3 (pr√°ctica):** Dejar 35 min completos para experimentar
- **Buffer:** Dejar 5 min de margen para Q&A

### Engagement
- Pedir a participantes que compartan sus m√©tricas en chat
- Hacer polls: "¬øCu√°ntos obtuvieron latencia < 2000ms?"
- Celebrar cuando alguien encuentra un edge case interesante

### Debugging en Vivo
- **No ocultar errores:** Si algo falla, √∫salo como ense√±anza
- **Pensar en voz alta:** "Hmm, latencia de 5 segundos, revisemos K..."
- **Involucrar participantes:** "¬øAlguien tiene idea de por qu√© fall√≥?"

### Adaptaciones por Nivel
- **Si van r√°pido:** Introducir desaf√≠os (Celda 18)
- **Si van lentos:** Saltear an√°lisis detallado de m√©tricas, ir directo a pr√°ctica
- **Grupo mixto:** Emparejar avanzados con principiantes

---

## üìö Recursos y Referencias

### Para compartir con participantes:
- [ChromaDB Docs](https://docs.trychroma.com/)
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [PyPDF2 Tutorial](https://pypdf2.readthedocs.io/)

### Lecturas complementarias (post-workshop):
- [Building RAG Applications](https://www.anthropic.com/index/building-effective-agents)
- [Vector Databases Explained](https://www.pinecone.io/learn/vector-database/)
- [Chunking Strategies](https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex)

---

## üéØ Criterios de Evaluaci√≥n

### Evaluaci√≥n Individual (autom√°tica)
```python
evaluation = TestSuite.evaluate_response(respuesta, Module.BASICS)
```

**Criterios:**
- Score ‚â• 0.70 ‚Üí ‚úÖ Aprobado
- Score < 0.70 ‚Üí ‚ö†Ô∏è Revisar

### Evaluaci√≥n Grupal (instructor)
- **Comprensi√≥n conceptual:** ¬øPueden explicar las 3 fases?
- **Habilidad t√©cnica:** ¬øEjecutaron todas las celdas exitosamente?
- **Experimentaci√≥n:** ¬øProbaron diferentes queries y par√°metros?
- **An√°lisis cr√≠tico:** ¬øIdentificaron limitaciones del sistema?

---

## ‚úÖ Checklist del Instructor

**Antes del m√≥dulo:**
- [ ] Probar notebook completo desde cero
- [ ] Verificar que company_handbook.pdf tiene contenido √∫til
- [ ] Preparar 3-5 queries de ejemplo interesantes
- [ ] Tener ChromaDB troubleshooting guide a mano

**Durante el m√≥dulo:**
- [ ] Explicar arquitectura RAG con diagrama (Celda 4)
- [ ] Codear en vivo SimpleRAG (no solo mostrar)
- [ ] Demostrar al menos 5 queries diferentes
- [ ] Mostrar caso de alucinaci√≥n (Celda 14)
- [ ] Revisar m√©tricas grupales al final

**Despu√©s del m√≥dulo:**
- [ ] Anotar queries que funcionaron bien/mal
- [ ] Identificar puntos de confusi√≥n comunes
- [ ] Actualizar ejemplos para pr√≥xima edici√≥n
- [ ] Verificar que todos est√°n listos para M√≥dulo 2

---

**üöÄ ¬°√âxito con el m√≥dulo!**
