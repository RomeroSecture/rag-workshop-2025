# üéì Gu√≠a del Instructor - Notebook 03: Frameworks Profesionales
## Choose Your Path: LangChain vs LlamaIndex

---

## üìã Informaci√≥n General

**Duraci√≥n:** 120 minutos (12:30-15:30 con comida 14:00-15:00)
- **Teor√≠a:** 45 min (12:30-13:15)
- **Pr√°ctica Parte 1:** 45 min (13:15-14:00)
- **Comida:** 60 min (14:00-15:00)
- **Pr√°ctica Parte 2:** 30 min (15:00-15:30)

**Objetivo:** Implementar RAG con frameworks profesionales y elegir el adecuado
**Nivel:** Avanzado
**Pre-requisitos:** Haber completado M√≥dulos 1 y 2

---

## üéØ Objetivos de Aprendizaje

Al finalizar este m√≥dulo, los participantes ser√°n capaces de:

1. ‚úÖ **Comparar** LangChain vs LlamaIndex (pros/cons)
2. ‚úÖ **Elegir** el framework adecuado seg√∫n caso de uso
3. ‚úÖ **Implementar** RAG con el framework elegido
4. ‚úÖ **A√±adir** memoria conversacional
5. ‚úÖ **Integrar** agents y tools
6. ‚úÖ **Alcanzar** 800ms de latencia (-20% vs M√≥dulo 2)

---

## üìä M√©tricas Target

| M√©trica | M√≥dulo 2 (Baseline) | M√≥dulo 3 (Target) | Mejora |
|---------|---------------------|-------------------|---------|
| ‚è±Ô∏è Latencia | 1000ms | 800ms | -20% |
| üí∞ Costo | $0.008 | $0.006 | -25% |
| üéØ Accuracy | 80% | 85% | +6% |
| üîß Mantenibilidad | Media | Alta | +++ |

---

## üóìÔ∏è Timeline Detallado

| Tiempo | Secci√≥n | Actividad | Celdas |
|--------|---------|-----------|--------|
| 12:30-12:45 | Parte 1: Comparaci√≥n | LangChain vs LlamaIndex demos | 1-5 |
| 12:45-13:00 | Parte 2: Decisi√≥n | Elegir path (A/B/C) | 6 |
| 13:00-13:45 | Parte 3: Implementaci√≥n | Path elegido en profundidad | 7-11 |
| 13:45-14:00 | Parte 4: Features | Memory, Agents, Tools | 12-13 |
| **14:00-15:00** | **üçΩÔ∏è COMIDA** | **Break largo** | - |
| 15:00-15:30 | Parte 5: Finalizaci√≥n | Benchmark y comparaci√≥n | 14-16 |

---

## üìù Gui√≥n de la Sesi√≥n

### PARTE 1: Comparaci√≥n Side-by-Side [12:30-12:45] - 15 min

#### 1. Introducci√≥n a Frameworks (3 min)

**Instructor presenta:**

> "Hasta ahora hemos construido todo desde cero para entender los fundamentos. Ahora vamos a usar frameworks profesionales que abstraen mucha complejidad. Los dos principales son LangChain y LlamaIndex. Vamos a compararlos implementando el MISMO RAG con ambos."

**Conceptos clave:**
- **Framework = Biblioteca con componentes pre-construidos**
- **Abstracci√≥n = Menos c√≥digo, m√°s productividad**
- **Trade-off = Flexibilidad vs Simplicidad**

**Ejecutar Celda 1:**
```python
print("üîç COMPARACI√ìN: LangChain vs LlamaIndex")
```

**Salida esperada:**
```
‚úÖ LangChain disponible
‚úÖ LlamaIndex disponible
```

**Si faltan frameworks:**
```bash
pip install langchain langchain-openai
pip install llama-index
```

#### 2. Demo LangChain (5 min)

**Instructor ejecuta Celda 2:**

```python
def langchain_demo():
    """RAG con LangChain en 10 l√≠neas"""
    # 1. Cargar documento
    loader = PyPDFLoader("../data/company_handbook.pdf")
    documents = loader.load()

    # 2. Split
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    texts = text_splitter.split_documents(documents)

    # 3. Vectorstore
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(texts, embeddings)

    # 4. Chain
    qa_chain = RetrievalQA.from_chain_type(
        llm=OpenAI(temperature=0),
        retriever=vectorstore.as_retriever()
    )

    # 5. Query
    result = qa_chain.run("¬øCu√°l es la pol√≠tica de vacaciones?")
    return result
```

**Explicar paso a paso:**
- **Loader:** Abstrae la lectura de PDFs
- **TextSplitter:** Chunking inteligente autom√°tico
- **Vectorstore:** Wrapper de ChromaDB con embeddings
- **Chain:** Pipeline completo de retrieval + QA
- **Query:** Una sola llamada para todo

**‚è±Ô∏è Timing esperado:** ~2-3 segundos primera ejecuci√≥n

#### 3. Demo LlamaIndex (5 min)

**Instructor ejecuta Celda 3:**

```python
def llamaindex_demo():
    """RAG con LlamaIndex en 5 l√≠neas"""
    # 1. Cargar documentos
    documents = SimpleDirectoryReader('../data').load_data()

    # 2. Crear √≠ndice
    index = VectorStoreIndex.from_documents(documents)

    # 3. Query engine
    query_engine = index.as_query_engine(
        llm=LlamaOpenAI(temperature=0)
    )

    # 4. Query
    response = query_engine.query("¬øCu√°l es la pol√≠tica de vacaciones?")
    return str(response)
```

**Explicar diferencias clave:**
- **Menos l√≠neas:** 5 vs 10
- **M√°s autom√°tico:** √çndice se crea solo
- **Menos configuraci√≥n:** Defaults inteligentes
- **Focus en b√∫squeda:** Optimizado para retrieval

**üí° Insight para compartir:**
> "LangChain da m√°s control, LlamaIndex da m√°s rapidez. Depende de tu necesidad."

#### 4. Tabla Comparativa (2 min)

**Instructor ejecuta Celda 5:**

Mostrar tabla de comparaci√≥n:

| Aspecto | LangChain | LlamaIndex |
|---------|-----------|------------|
| **Filosof√≠a** | Orquestaci√≥n | Indexaci√≥n |
| **Fortaleza** | Chains complejas, agents | B√∫squeda avanzada |
| **Curva aprendizaje** | Media-Alta | Media |
| **L√≠neas de c√≥digo** | +++ | + |
| **Flexibilidad** | Muy alta | Alta |
| **Ecosistema** | 250+ integr. | 100+ integr. |
| **Mejor para** | Pipelines complejos | B√∫squeda optimizada |
| **Documentaci√≥n** | Extensa | Muy buena |

**Explicar cada aspecto:**
- **Filosof√≠a:** LangChain orquesta, LlamaIndex indexa
- **Fortaleza:** LangChain para apps complejas, LlamaIndex para b√∫squeda
- **Ecosistema:** Ambos tienen muchas integraciones

---

### PARTE 2: Elige Tu Camino [12:45-13:00] - 15 min

#### 5. Metodolog√≠a "Choose Your Path" (5 min)

**Instructor presenta (Celda 6):**

> "Ahora viene la parte interesante. Cada uno va a elegir un camino:
> - **Path A: LangChain** - Si necesitas agents, chains complejas, orquestaci√≥n
> - **Path B: LlamaIndex** - Si necesitas b√∫squeda avanzada, multi-√≠ndices
> - **Path C: Hybrid** - Si quieres combinar lo mejor de ambos
>
> No hay respuesta correcta. Depende de tu caso de uso."

**Gu√≠a para ayudar a elegir:**

**Elige LangChain si:**
- Necesitas agents con herramientas externas
- Quieres chains complejas multi-paso
- Requieres mucha personalizaci√≥n
- Tu app tiene workflows complejos
- Ejemplo: Chatbot que consulta APIs + base de datos + RAG

**Elige LlamaIndex si:**
- Enfocado principalmente en b√∫squeda
- Quieres prototipado r√°pido
- Necesitas query engines sofisticados
- Tu app es mayormente RAG puro
- Ejemplo: Sistema de b√∫squeda documental

**Elige Hybrid si:**
- Quieres lo mejor de ambos
- Sistema enterprise complejo
- Equipo grande con especializaci√≥n
- Ejemplo: Plataforma con b√∫squeda (LlamaIndex) + automation (LangChain)

**Hacer poll:**
```
Por favor levanten la mano:
- Path A (LangChain): [contar]
- Path B (LlamaIndex): [contar]
- Path C (Hybrid): [contar]
```

**Anotar distribuci√≥n para ajustar timing.**

#### 6. Preparaci√≥n de Datos (5 min)

**IMPORTANTE:** Antes de elegir path, todos deben ejecutar celda de preparaci√≥n:

```python
# Preparar chunks para todos los paths
from module_2_optimized import Module2_OptimizedRAG
rag_base = Module2_OptimizedRAG()
doc = rag_base.load_document()
chunks = rag_base.create_chunks(doc)
```

**Verificar que todos tienen chunks listos antes de continuar.**

#### 7. Dividir la Sala (5 min)

**Estrategia seg√∫n distribuci√≥n:**

**Si mayor√≠a elige un path (>60%):**
- Ense√±ar ese path en detalle (30 min)
- Mostrar los otros paths brevemente (10 min cada uno)

**Si distribuci√≥n equilibrada:**
- Ense√±ar Path A en detalle (20 min)
- Ense√±ar Path B en detalle (20 min)
- Mostrar Path C brevemente (5 min)

**Si hay grupos peque√±os (<3 personas):**
- Sugerir unirse a un grupo m√°s grande
- Ofrecer ayuda individual durante pr√°ctica

---

### PARTE 3: Implementaci√≥n Profunda [13:00-13:45] - 45 min

#### PATH A: LangChain Completo (Si mayor√≠a elige este)

**Instructor ejecuta Celda 8 paso a paso:**

##### 1. Clase Base (10 min)

```python
class Module3_LangChainRAG(Module2_OptimizedRAG):
    """RAG con LangChain - Extiende M2"""

    def __init__(self, initial_chunks=None):
        super().__init__()
        self.module = "M3_LangChain"

        if initial_chunks:
            self.chunks = initial_chunks

        self.setup_langchain()
        self.setup_agent()
```

**Explicar herencia:**
- Extendemos Module2_OptimizedRAG
- Mantenemos optimizaciones previas
- A√±adimos funcionalidad LangChain

##### 2. Setup LangChain (10 min)

```python
def setup_langchain(self):
    # Embeddings
    self.embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small"
    )

    # Vector Store
    self.vectorstore = Chroma(
        collection_name="m3_langchain",
        embedding_function=self.embeddings
    )

    # A√±adir chunks de M2
    if self.chunks:
        texts = [chunk if isinstance(chunk, str) else
                chunk.get('text', str(chunk)) for chunk in self.chunks]
        self.vectorstore.add_texts(texts)

    # QA Chain
    self.qa_chain = RetrievalQA.from_chain_type(
        llm=OpenAI(temperature=0.3),
        chain_type="stuff",
        retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )
```

**Conceptos a explicar:**
- **Embeddings:** Usamos modelo de OpenAI
- **Vectorstore:** ChromaDB con funci√≥n de embeddings
- **RetrievalQA:** Chain pre-construida de LangChain
- **chain_type="stuff":** Mete todo el contexto en el prompt

##### 3. Agents y Tools (15 min) ‚≠ê **FEATURE CLAVE**

```python
def setup_agent(self):
    # Definir tools
    tools = [
        Tool(
            name="Company_QA",
            func=self.qa_chain.run,
            description="√ötil para responder preguntas sobre pol√≠ticas de la empresa"
        ),
        Tool(
            name="Calculator",
            func=lambda x: str(eval(x)),
            description="√ötil para c√°lculos matem√°ticos"
        )
    ]

    # Inicializar agent
    self.agent = initialize_agent(
        tools,
        OpenAI(temperature=0),
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        memory=self.memory
    )
```

**Explicar Agents en detalle:**

**¬øQu√© es un Agent?**
- Sistema que decide qu√© herramienta usar
- Loop: Observar ‚Üí Pensar ‚Üí Actuar ‚Üí Repetir
- Puede usar m√∫ltiples tools en secuencia

**Ejemplo de razonamiento del agent:**
```
Pregunta: "Si cada empleado tiene 22 d√≠as y somos 50, ¬øcu√°ntos d√≠as totales?"

Agent piensa:
1. Necesito saber cu√°ntos d√≠as tiene cada empleado ‚Üí Usar Company_QA
2. Tengo la respuesta: 22 d√≠as
3. Necesito multiplicar 22 * 50 ‚Üí Usar Calculator
4. Respuesta final: 1,100 d√≠as
```

**Demo en vivo:**
```python
rag_langchain = Module3_LangChainRAG(initial_chunks=chunks)

queries = [
    "¬øCu√°ntos d√≠as de vacaciones tienen los empleados?",
    "¬øY si tienen 5 a√±os de antig√ºedad?",  # Usa memoria
    "Calcula cu√°ntos d√≠as ser√≠an en 3 a√±os"  # Usa calculadora
]

for q in queries:
    response = rag_langchain.query_with_memory(q)
    print(f"Q: {q}\nA: {response}\n")
```

**Salida esperada (con verbose=True):**
```
> Entering new AgentExecutor chain...
I need to find information about vacation policies
Action: Company_QA
Action Input: vacation days for employees
Observation: Employees get 22 days...
Thought: Now I know the answer
Final Answer: Los empleados tienen 22 d√≠as h√°biles...
```

**Explicar cada parte del output:**
- **Thought:** El agent razona
- **Action:** Elige qu√© tool usar
- **Observation:** Resultado del tool
- **Final Answer:** Respuesta final

##### 4. Memoria Conversacional (10 min)

```python
self.memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)
```

**Demo de memoria:**
```python
# Primera pregunta
"¬øCu√°ntos d√≠as de vacaciones tienen?"
# Respuesta: 22 d√≠as

# Segunda pregunta (referencia a la primera)
"¬øY despu√©s de 5 a√±os?"
# El agent recuerda el contexto y responde: 25 d√≠as
```

**Tipos de memoria en LangChain:**
- **ConversationBufferMemory:** Guarda todo
- **ConversationSummaryMemory:** Resume conversaciones largas
- **ConversationBufferWindowMemory:** Solo √∫ltimas N interacciones

#### PATH B: LlamaIndex Completo (Si mayor√≠a elige este)

**Instructor ejecuta Celda 10 paso a paso:**

##### 1. Clase Base (5 min)

```python
class Module3_LlamaIndexRAG(Module2_OptimizedRAG):
    """RAG con LlamaIndex - Extiende M2"""

    def __init__(self, initial_chunks=None):
        super().__init__()
        if initial_chunks:
            self.chunks = initial_chunks
        self.setup_llamaindex()
```

##### 2. Setup LlamaIndex (10 min)

```python
def setup_llamaindex(self):
    # Convertir chunks a Documents
    documents = []
    for i, chunk in enumerate(self.chunks):
        text = chunk if isinstance(chunk, str) else chunk.get('text', str(chunk))
        doc = Document(
            text=text,
            metadata={"chunk_id": i, "source": "company_handbook"}
        )
        documents.append(doc)

    # Crear √≠ndice
    self.index = VectorStoreIndex.from_documents(
        documents,
        embed_model=OpenAIEmbedding(model="text-embedding-3-small")
    )

    # Query engine
    self.query_engine = self.index.as_query_engine(
        llm=LlamaOpenAI(model="gpt-3.5-turbo", temperature=0.3),
        similarity_top_k=3,
        streaming=False
    )
```

**Conceptos LlamaIndex:**
- **Document:** Objeto de LlamaIndex para texto + metadata
- **VectorStoreIndex:** √çndice vectorial autom√°tico
- **QueryEngine:** Motor de b√∫squeda + generaci√≥n

##### 3. Query con Contexto Mejorado (10 min)

```python
def query_with_context(self, question: str):
    response = self.query_engine.query(question)

    # Extraer fuentes
    sources = []
    if hasattr(response, 'source_nodes'):
        for node in response.source_nodes:
            sources.append({
                "text": node.text[:100],
                "score": getattr(node, 'score', 0.0)
            })

    return {
        "answer": str(response),
        "sources": sources
    }
```

**Ventaja de LlamaIndex:** Source tracking autom√°tico

##### 4. Features Avanzadas (20 min)

**Re-ranking con LlamaIndex:**
```python
from llama_index.indices.postprocessor import SentenceTransformerRerank

self.reranker = SentenceTransformerRerank(
    model="cross-encoder/ms-marco-MiniLM-L-2-v2",
    top_n=2
)

self.query_engine = self.index.as_query_engine(
    similarity_top_k=5,
    node_postprocessors=[self.reranker]
)
```

**Explicar re-ranking:**
- Primer paso: Embedding similarity (top 5)
- Segundo paso: Cross-encoder rerank (mejores 2)
- Resultado: Mayor precisi√≥n

**Demo:**
```python
rag_llamaindex = Module3_LlamaIndexRAG(initial_chunks=chunks)

result = rag_llamaindex.query_with_context("¬øCu√°l es la pol√≠tica de trabajo remoto?")
print(f"Respuesta: {result['answer']}")
print(f"Fuentes: {len(result['sources'])} documentos")
for i, source in enumerate(result['sources']):
    print(f"  {i+1}. Score: {source['score']:.3f} - {source['text'][:80]}...")
```

#### PATH C: Hybrid (Explicaci√≥n breve - 10 min)

**Instructor ejecuta Celda 12:**

**Concepto:**
- LlamaIndex para indexaci√≥n/b√∫squeda
- LangChain para orquestaci√≥n/agents

```python
class Module3_HybridRAG:
    def setup_llamaindex_indexing(self):
        # LlamaIndex crea el √≠ndice
        self.llama_index = VectorStoreIndex.from_documents(documents)

    def setup_langchain_orchestration(self):
        # LangChain usa LlamaIndex como tool
        def search_with_llamaindex(query: str) -> str:
            return str(self.llama_index.as_query_engine().query(query))

        self.llama_tool = Tool(
            name="LlamaIndex_Search",
            func=search_with_llamaindex,
            description="B√∫squeda optimizada"
        )
```

**Cu√°ndo usar Hybrid:**
- Apps muy grandes
- Equipos especializados
- Lo mejor de ambos mundos
- Trade-off: M√°s complejidad

---

### PARTE 4: Features Avanzadas [13:45-14:00] - 15 min

#### 8. Comparaci√≥n de Features (10 min)

**Ejecutar Celda 15:**

Comparar m√©tricas de los 3 approaches:

| Framework | Latencia | L√≠neas c√≥digo | Flexibilidad | Mejor para |
|-----------|----------|---------------|--------------|------------|
| LangChain | 850ms | 150 | Alta | Pipelines complejos |
| LlamaIndex | 750ms | 100 | Media | B√∫squeda optimizada |
| Hybrid | 800ms | 200 | Muy Alta | Enterprise |

**Discusi√≥n guiada:**
> "¬øQu√© observan? LlamaIndex es m√°s r√°pido porque est√° optimizado para b√∫squeda. LangChain da m√°s control. Hybrid combina ambos pero a√±ade complejidad."

#### 9. Decisi√≥n Final (5 min)

**Preguntas para el grupo:**
1. "¬øQui√©n usar√≠a LangChain en su proyecto? ¬øPor qu√©?"
2. "¬øQui√©n usar√≠a LlamaIndex? ¬øPor qu√©?"
3. "¬øAlguien necesitar√≠a Hybrid?"

**Casos de uso reales:**
- **LangChain:** Chatbot con acceso a 5 APIs diferentes
- **LlamaIndex:** Sistema de b√∫squeda en 1M documentos
- **Hybrid:** Plataforma con b√∫squeda + automatizaci√≥n

---

## üçΩÔ∏è COMIDA [14:00-15:00] - 60 min

**Instructor anuncia:**
> "¬°Excelente trabajo esta ma√±ana! Hora de recargar energ√≠as. Comida de 14:00 a 15:00. Volvemos puntuales a las 15:00 para terminar el M√≥dulo 3 y comenzar producci√≥n."

**Durante la comida:**
- Estar disponible para preguntas
- Networking entre participantes
- Revisar notebooks de quienes van atrasados

---

### PARTE 5: Finalizaci√≥n y Benchmark [15:00-15:30] - 30 min

#### 10. Completar Implementaciones (15 min)

**Para quienes no terminaron:**
- Ayudar a completar su path elegido
- Asegurar que todos tienen algo funcionando

**Para quienes terminaron:**
- Experimentar con el otro path
- Comparar ambos approaches
- Optimizar par√°metros

#### 11. Benchmark Final (10 min)

**Ejecutar queries de prueba con ambos frameworks:**

```python
test_queries = [
    "¬øPol√≠tica de vacaciones?",
    "¬øBeneficios de empleados?",
    "¬øProceso de onboarding?"
]

# Comparar tiempos
for query in test_queries:
    # LangChain
    start = time.time()
    result_lc = rag_langchain.query(query)
    time_lc = (time.time() - start) * 1000

    # LlamaIndex
    start = time.time()
    result_li = rag_llamaindex.query(query)
    time_li = (time.time() - start) * 1000

    print(f"{query}: LC={time_lc:.0f}ms, LI={time_li:.0f}ms")
```

#### 12. Resumen y Conclusiones (5 min)

**Instructor ejecuta Celda 16:**

**Logros del m√≥dulo:**
1. ‚úÖ Compararon frameworks profesionales
2. ‚úÖ Eligieron seg√∫n caso de uso
3. ‚úÖ Implementaron con framework elegido
4. ‚úÖ A√±adieron memoria y agents
5. ‚úÖ Alcanzaron ~800ms de latencia

**M√©tricas finales:**
- Latencia: 1000ms ‚Üí 800ms (-20%)
- C√≥digo: -40% l√≠neas vs custom
- Mantenibilidad: +++ (librer√≠as con soporte)

**Transici√≥n a M√≥dulo 4:**
> "Ahora tienen RAG con frameworks profesionales. Falta el √∫ltimo paso: llevarlo a producci√≥n. FastAPI, Docker, deployment. ¬°Vamos!"

---

## üìä M√©tricas de √âxito del M√≥dulo

Al final del m√≥dulo, verificar:

- [ ] **90%+** eligieron un path y lo implementaron
- [ ] **80%+** entienden diferencias LangChain vs LlamaIndex
- [ ] **75%+** tienen agents/memory funcionando
- [ ] **Latencia promedio grupal** < 900ms
- [ ] **Accuracy promedio** > 0.83

---

## üö® Problemas Comunes y Soluciones

### 1. Frameworks no instalados
**S√≠ntomas:** `ModuleNotFoundError`
**Soluci√≥n:**
```bash
pip install langchain langchain-openai langchain-community
pip install llama-index llama-index-llms-openai llama-index-embeddings-openai
```

### 2. Agent no encuentra tools
**S√≠ntomas:** Agent dice "I don't have access to that information"
**Causa:** Tool description no es clara
**Soluci√≥n:**
```python
Tool(
    name="Company_QA",
    func=self.qa_chain.run,
    description="Use this tool to answer questions about company policies, benefits, vacation days, remote work, and HR topics. Input should be a question."
)
```

### 3. Memoria no funciona
**S√≠ntomas:** Agent no recuerda conversaciones previas
**Causa:** Memory no est√° en el agent
**Soluci√≥n:**
```python
self.agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    memory=self.memory,  # ‚Üê IMPORTANTE
    verbose=True
)
```

### 4. LlamaIndex lento
**S√≠ntomas:** Queries tardan >5 segundos
**Causa:** Indexando en cada query
**Soluci√≥n:**
```python
# Crear √≠ndice UNA VEZ
if not hasattr(self, 'index'):
    self.index = VectorStoreIndex.from_documents(documents)

# Usar √≠ndice existente
query_engine = self.index.as_query_engine()
```

### 5. Chunks no se transfieren
**S√≠ntomas:** "No tengo informaci√≥n" en queries
**Causa:** Chunks de M2 no llegaron al framework
**Soluci√≥n:**
```python
# Verificar chunks
print(f"Chunks recibidos: {len(self.chunks) if hasattr(self, 'chunks') else 0}")

# Agregar a vectorstore
if self.chunks:
    texts = [str(chunk) for chunk in self.chunks]
    self.vectorstore.add_texts(texts)
```

---

## üí° Tips de Ense√±anza

### Gesti√≥n de Paths
- **Si >70% elige un path:** Ense√±ar ese en profundidad, otros brevemente
- **Si distribuci√≥n 50/50:** Ense√±ar ambos paths principales
- **Si todos eligen diferente:** Ense√±ar LangChain (m√°s did√°ctico)

### Timing de la Comida
- **CR√çTICO:** Empezar comida a las 14:00 exactas
- Avisar 5 min antes (13:55)
- Volver puntual a las 15:00

### Demos Efectivas
- **Agents:** Mostrar verbose=True para ver razonamiento
- **Memoria:** Hacer 3 queries seguidas para demostrar
- **Re-ranking:** Mostrar scores antes y despu√©s

### Engagement
- Poll en vivo de qu√© path eligen
- Compartir pantallas de participantes con implementaciones interesantes
- Crear competencia amistosa: "¬øQui√©n logra latencia m√°s baja?"

---

## üìö Recursos Adicionales

### Para compartir con participantes:
- [LangChain Docs](https://python.langchain.com/)
- [LlamaIndex Docs](https://docs.llamaindex.ai/)
- [LangChain vs LlamaIndex Comparison](https://blog.langchain.dev/langchain-vs-llamaindex/)
- [Agents Deep Dive](https://python.langchain.com/docs/modules/agents/)

### Lecturas avanzadas:
- [Building Production Agents](https://www.anthropic.com/research/building-effective-agents)
- [LlamaIndex Query Engines](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/)
- [LangChain Expression Language](https://python.langchain.com/docs/expression_language/)

---

## ‚úÖ Checklist del Instructor

**Antes del m√≥dulo:**
- [ ] Verificar que LangChain y LlamaIndex est√°n instalados
- [ ] Probar ambos paths completos
- [ ] Preparar ejemplos de agents con verbose=True
- [ ] Tener backup de chunks preparados

**Durante el m√≥dulo:**
- [ ] Poll de qu√© path eligen
- [ ] Demo de agents con razonamiento visible
- [ ] Mostrar memoria conversacional en acci√≥n
- [ ] Comparar frameworks side-by-side
- [ ] Avisar comida 5 min antes

**Despu√©s del m√≥dulo:**
- [ ] Verificar que todos eligieron y completaron un path
- [ ] Recopilar feedback de frameworks
- [ ] Anotar qu√© path fue m√°s popular
- [ ] Preparar para M√≥dulo 4

---

**üöÄ ¬°Este es el m√≥dulo donde eligen su superpoder: LangChain o LlamaIndex!**
